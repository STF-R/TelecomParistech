{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18.1\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's use CoNLL 2002 data to build a NER system\n",
    "\n",
    "CoNLL2002 corpus is available in NLTK. We use Spanish data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'esp.testa',\n",
       " u'esp.testb',\n",
       " u'esp.train',\n",
       " u'ned.testa',\n",
       " u'ned.testb',\n",
       " u'ned.train']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.conll2002.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.93 s, sys: 28 ms, total: 1.96 s\n",
      "Wall time: 1.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))\n",
    "test_sents_2 = list(nltk.corpus.conll2002.iob_sents('esp.testa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Melbourne', u'NP', u'B-LOC'),\n",
       " (u'(', u'Fpa', u'O'),\n",
       " (u'Australia', u'NP', u'B-LOC'),\n",
       " (u')', u'Fpt', u'O'),\n",
       " (u',', u'Fc', u'O'),\n",
       " (u'25', u'Z', u'O'),\n",
       " (u'may', u'NC', u'O'),\n",
       " (u'(', u'Fpa', u'O'),\n",
       " (u'EFE', u'NC', u'B-ORG'),\n",
       " (u')', u'Fpt', u'O'),\n",
       " (u'.', u'Fp', u'O')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "Next, define some features. In this example we use word identity, word suffix, word shape and word POS tag; also, some information from nearby words is used. \n",
    "\n",
    "This makes a simple baseline, but you certainly can add and remove some features to get (much?) better results - experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'postag=' + postag,\n",
    "        'postag[:2]=' + postag[:2],\n",
    "    ]\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "            '-1:postag=' + postag1,\n",
    "            '-1:postag[:2]=' + postag1[:2],\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "            '+1:postag=' + postag1,\n",
    "            '+1:postag[:2]=' + postag1[:2],\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "                \n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what word2features extracts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bias',\n",
       " u'word.lower=melbourne',\n",
       " u'word[-3:]=rne',\n",
       " u'word[-2:]=ne',\n",
       " 'word.isupper=False',\n",
       " 'word.istitle=True',\n",
       " 'word.isdigit=False',\n",
       " u'postag=NP',\n",
       " u'postag[:2]=NP',\n",
       " 'BOS',\n",
       " u'+1:word.lower=(',\n",
       " '+1:word.istitle=False',\n",
       " '+1:word.isupper=False',\n",
       " u'+1:postag=Fpa',\n",
       " u'+1:postag[:2]=Fp']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(train_sents[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the features from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.02 s, sys: 504 ms, total: 7.52 s\n",
      "Wall time: 7.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "X_test_2 = [sent2features(s) for s in test_sents_2]\n",
    "y_test_2 = [sent2labels(s) for s in test_sents_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "To train the model, we create pycrfsuite.Trainer, load the training data and call 'train' method. \n",
    "First, create pycrfsuite.Trainer and load the training data to CRFsuite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.61 s, sys: 12 ms, total: 4.62 s\n",
      "Wall time: 4.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set training parameters. We will use L-BFGS training algorithm (it is default) with Elastic Net (L1 + L2) regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible parameters for the default training algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature.minfreq',\n",
       " 'feature.possible_states',\n",
       " 'feature.possible_transitions',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'max_iterations',\n",
       " 'num_memories',\n",
       " 'epsilon',\n",
       " 'period',\n",
       " 'delta',\n",
       " 'linesearch',\n",
       " 'max_linesearch']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 s, sys: 0 ns, total: 18 s\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train('conll2002-esp.crfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer.train saves model to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 matthieu matthieu 601K mars  17 22:01 ./conll2002-esp.crfsuite\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./conll2002-esp.crfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get information about the final state of the model by looking at the trainer's logparser. If we had tagged our input data using the optional group argument in add, and had used the optional holdout argument during train, there would be information about the trainer's performance on the holdout set as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'active_features': 11346,\n",
       " 'error_norm': 1262.912078,\n",
       " 'feature_norm': 79.110017,\n",
       " 'linesearch_step': 1.0,\n",
       " 'linesearch_trials': 1,\n",
       " 'loss': 14807.577946,\n",
       " 'num': 50,\n",
       " 'scores': {},\n",
       " 'time': 0.368}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.logparser.last_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get this information for every step using trainer.logparser.iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 {'loss': 14807.577946, 'error_norm': 1262.912078, 'linesearch_trials': 1, 'active_features': 11346, 'num': 50, 'time': 0.368, 'scores': {}, 'linesearch_step': 1.0, 'feature_norm': 79.110017}\n"
     ]
    }
   ],
   "source": [
    "print len(trainer.logparser.iterations), trainer.logparser.iterations[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "To use the trained model, create pycrfsuite.Tagger, open the model and use \"tag\" method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7f4de43f1750>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('conll2002-esp.crfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tag a sentence to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La Coruña , 23 may ( EFECOM ) .\n",
      "\n",
      "\n",
      "('Predicted:', 'B-LOC I-LOC O O O O B-ORG O O')\n",
      "('Correct:  ', u'B-LOC I-LOC O O O O B-ORG O O')\n"
     ]
    }
   ],
   "source": [
    "example_sent = test_sents[0]\n",
    "print(' '.join(sent2tokens(example_sent)) + '\\n\\n')\n",
    "\n",
    "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))\n",
    "print(\"Correct:  \", ' '.join(sent2labels(example_sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bio_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict entity labels for all sentences in our testing set ('testb' Spanish data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 528 ms, sys: 12 ms, total: 540 ms\n",
      "Wall time: 538 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = [tagger.tag(xseq) for xseq in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..and check the result. Note this report is not comparable to results in CONLL2002 papers because here we check per-token results (not per-entity). Per-entity numbers will be worse.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.78      0.75      0.76      1084\n",
      "      I-LOC       0.66      0.60      0.63       325\n",
      "     B-MISC       0.69      0.47      0.56       339\n",
      "     I-MISC       0.61      0.49      0.54       557\n",
      "      B-ORG       0.79      0.81      0.80      1400\n",
      "      I-ORG       0.80      0.79      0.80      1104\n",
      "      B-PER       0.82      0.87      0.84       735\n",
      "      I-PER       0.87      0.93      0.90       634\n",
      "\n",
      "avg / total       0.77      0.76      0.76      6178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bio_classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check what classifier learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "B-ORG  -> I-ORG   8.631963\n",
      "I-ORG  -> I-ORG   7.833706\n",
      "B-PER  -> I-PER   6.998706\n",
      "B-LOC  -> I-LOC   6.913675\n",
      "I-MISC -> I-MISC  6.129735\n",
      "B-MISC -> I-MISC  5.538291\n",
      "I-LOC  -> I-LOC   4.983567\n",
      "I-PER  -> I-PER   3.748358\n",
      "B-ORG  -> B-LOC   1.727090\n",
      "B-PER  -> B-LOC   1.388267\n",
      "B-LOC  -> B-LOC   1.240278\n",
      "O      -> O       1.197929\n",
      "O      -> B-ORG   1.097062\n",
      "I-PER  -> B-LOC   1.083332\n",
      "O      -> B-MISC  1.046113\n",
      "\n",
      "Top unlikely transitions:\n",
      "I-PER  -> B-ORG   -2.056130\n",
      "I-LOC  -> I-ORG   -2.143940\n",
      "B-ORG  -> I-MISC  -2.167501\n",
      "I-PER  -> I-ORG   -2.369380\n",
      "B-ORG  -> I-PER   -2.378110\n",
      "I-MISC -> I-PER   -2.458788\n",
      "B-LOC  -> I-PER   -2.516414\n",
      "I-ORG  -> I-MISC  -2.571973\n",
      "I-LOC  -> B-PER   -2.697791\n",
      "I-LOC  -> I-PER   -3.065950\n",
      "I-ORG  -> I-PER   -3.364434\n",
      "O      -> I-PER   -7.322841\n",
      "O      -> I-MISC  -7.648246\n",
      "O      -> I-ORG   -8.024126\n",
      "O      -> I-LOC   -8.333815\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "info = tagger.info()\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common(15))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common()[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, for example, it is very likely that the beginning of an organization name (B-ORG) will be followed by a token inside organization name (I-ORG), but transitions to I-ORG from tokens with other labels are penalized. Also note I-PER -> B-LOC transition: a positive weight means that model thinks that a person name is often followed by a location.\n",
    "\n",
    "Check the state features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "8.886516 B-ORG  word.lower=efe-cantabria\n",
      "8.743642 B-ORG  word.lower=psoe-progresistas\n",
      "5.769032 B-LOC  -1:word.lower=cantabria\n",
      "5.195429 I-LOC  -1:word.lower=calle\n",
      "5.116821 O      word.lower=mayo\n",
      "4.990871 O      -1:word.lower=día\n",
      "4.910915 I-ORG  -1:word.lower=l\n",
      "4.721572 B-MISC word.lower=diversia\n",
      "4.676259 B-ORG  word.lower=telefónica\n",
      "4.334354 B-ORG  word[-2:]=-e\n",
      "4.149862 B-ORG  word.lower=amena\n",
      "4.141370 B-ORG  word.lower=terra\n",
      "3.942852 O      word.istitle=False\n",
      "3.926397 B-ORG  word.lower=continente\n",
      "3.924672 B-ORG  word.lower=acesa\n",
      "3.888706 O      word.lower=euro\n",
      "3.856445 B-PER  -1:word.lower=según\n",
      "3.812373 B-MISC word.lower=exteriores\n",
      "3.807582 I-MISC -1:word.lower=1.9\n",
      "3.807098 B-MISC word.lower=sanidad\n",
      "\n",
      "Top negative:\n",
      "-1.965379 O      word.lower=fundación\n",
      "-1.981541 O      -1:word.lower=británica\n",
      "-2.118347 O      word.lower=061\n",
      "-2.190653 B-PER  word[-3:]=nes\n",
      "-2.226373 B-ORG  postag=SP\n",
      "-2.226373 B-ORG  postag[:2]=SP\n",
      "-2.260972 O      word[-3:]=uia\n",
      "-2.384920 O      -1:word.lower=sección\n",
      "-2.483009 O      word[-2:]=s.\n",
      "-2.535050 I-LOC  BOS\n",
      "-2.583123 O      -1:word.lower=sánchez\n",
      "-2.585756 O      postag[:2]=NP\n",
      "-2.585756 O      postag=NP\n",
      "-2.588899 O      word[-2:]=om\n",
      "-2.738583 O      -1:word.lower=carretera\n",
      "-2.913103 O      word.istitle=True\n",
      "-2.926560 O      word[-2:]=nd\n",
      "-2.946862 I-PER  -1:word.lower=san\n",
      "-2.954094 B-PER  -1:word.lower=del\n",
      "-3.529449 O      word.isupper=True\n"
     ]
    }
   ],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-6s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(info.state_features).most_common(20))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(info.state_features).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "\n",
    "* **8.743642 B-ORG  word.lower=psoe-progresistas** - the model remembered names of some entities - maybe it is overfit, or maybe our features are not adequate, or maybe remembering is indeed helpful;\n",
    "* **5.195429 I-LOC  -1:word.lower=calle**: \"calle\" is a street in Spanish; model learns that if a previous word was \"calle\" then the token is likely a part of location;\n",
    "* **-3.529449 O      word.isupper=True**, ** -2.913103 O      word.istitle=True **: UPPERCASED or TitleCased words are likely entities of some kind;\n",
    "* **-2.585756 O      postag=NP** - proper nouns (NP is a proper noun in the Spanish tagset) are often entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP - CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pickling  test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "pickle.dump({'X': X_test, 'y': y_test}, open('../conll2002/conll2002-esp_crfsuite-test-data.dump', 'wb'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hyper-parameter tuning\n",
    "\n",
    "In the following we will test different values for the two regularization l1 and l2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1: 0.0e+00 - c2: 0.0e+00 \n",
      " Nb non-zero features : 96180 & Coefficient vector norm 146.9 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.79      0.74      0.76      1084\n",
      "      I-LOC       0.65      0.60      0.62       325\n",
      "     B-MISC       0.58      0.55      0.57       339\n",
      "     I-MISC       0.65      0.60      0.62       557\n",
      "      B-ORG       0.79      0.81      0.80      1400\n",
      "      I-ORG       0.85      0.75      0.79      1104\n",
      "      B-PER       0.82      0.87      0.84       735\n",
      "      I-PER       0.88      0.93      0.90       634\n",
      "\n",
      "avg / total       0.78      0.76      0.77      6178\n",
      "\n",
      "c1: 0.0e+00 - c2: 1.0e-04 \n",
      " Nb non-zero features : 96180 & Coefficient vector norm 113.9 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.78      0.77      0.78      1084\n",
      "      I-LOC       0.59      0.62      0.60       325\n",
      "     B-MISC       0.64      0.53      0.58       339\n",
      "     I-MISC       0.68      0.57      0.62       557\n",
      "      B-ORG       0.80      0.80      0.80      1400\n",
      "      I-ORG       0.84      0.74      0.79      1104\n",
      "      B-PER       0.82      0.88      0.85       735\n",
      "      I-PER       0.89      0.93      0.91       634\n",
      "\n",
      "avg / total       0.78      0.76      0.77      6178\n",
      "\n",
      "c1: 0.0e+00 - c2: 1.0e-03 \n",
      " Nb non-zero features : 96180 & Coefficient vector norm 175.8 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.80      0.74      0.77      1084\n",
      "      I-LOC       0.71      0.60      0.65       325\n",
      "     B-MISC       0.60      0.54      0.57       339\n",
      "     I-MISC       0.69      0.57      0.63       557\n",
      "      B-ORG       0.79      0.82      0.80      1400\n",
      "      I-ORG       0.83      0.76      0.79      1104\n",
      "      B-PER       0.80      0.88      0.84       735\n",
      "      I-PER       0.88      0.93      0.91       634\n",
      "\n",
      "avg / total       0.79      0.76      0.77      6178\n",
      "\n",
      "c1: 0.0e+00 - c2: 1.0e-02 \n",
      " Nb non-zero features : 96180 & Coefficient vector norm 129.0 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.80      0.75      0.77      1084\n",
      "      I-LOC       0.67      0.61      0.64       325\n",
      "     B-MISC       0.63      0.52      0.57       339\n",
      "     I-MISC       0.64      0.54      0.59       557\n",
      "      B-ORG       0.79      0.82      0.80      1400\n",
      "      I-ORG       0.82      0.76      0.79      1104\n",
      "      B-PER       0.80      0.89      0.85       735\n",
      "      I-PER       0.89      0.94      0.91       634\n",
      "\n",
      "avg / total       0.78      0.76      0.77      6178\n",
      "\n",
      "c1: 0.0e+00 - c2: 1.0e-01 \n",
      " Nb non-zero features : 96180 & Coefficient vector norm 88.8 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.79      0.75      0.77      1084\n",
      "      I-LOC       0.68      0.64      0.66       325\n",
      "     B-MISC       0.67      0.50      0.58       339\n",
      "     I-MISC       0.68      0.55      0.61       557\n",
      "      B-ORG       0.79      0.82      0.81      1400\n",
      "      I-ORG       0.82      0.79      0.80      1104\n",
      "      B-PER       0.83      0.88      0.86       735\n",
      "      I-PER       0.89      0.94      0.91       634\n",
      "\n",
      "avg / total       0.79      0.77      0.78      6178\n",
      "\n",
      "c1: 0.0e+00 - c2: 1.0e+00 \n",
      " Nb non-zero features : 96180 & Coefficient vector norm 57.8 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.79      0.76      0.78      1084\n",
      "      I-LOC       0.72      0.62      0.67       325\n",
      "     B-MISC       0.67      0.47      0.56       339\n",
      "     I-MISC       0.67      0.54      0.60       557\n",
      "      B-ORG       0.80      0.82      0.81      1400\n",
      "      I-ORG       0.83      0.79      0.81      1104\n",
      "      B-PER       0.81      0.89      0.85       735\n",
      "      I-PER       0.87      0.94      0.91       634\n",
      "\n",
      "avg / total       0.79      0.77      0.78      6178\n",
      "\n",
      "c1: 0.0e+00 - c2: 1.0e+01 \n",
      " Nb non-zero features : 96180 & Coefficient vector norm 24.1 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.71      0.69      0.70      1084\n",
      "      I-LOC       0.59      0.40      0.48       325\n",
      "     B-MISC       0.61      0.27      0.38       339\n",
      "     I-MISC       0.54      0.40      0.46       557\n",
      "      B-ORG       0.75      0.77      0.76      1400\n",
      "      I-ORG       0.74      0.79      0.76      1104\n",
      "      B-PER       0.76      0.83      0.79       735\n",
      "      I-PER       0.82      0.92      0.87       634\n",
      "\n",
      "avg / total       0.72      0.70      0.70      6178\n",
      "\n",
      "c1: 1.0e-04 - c2: 0.0e+00 \n",
      " Nb non-zero features : 37367 & Coefficient vector norm 931.0 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.76      0.75      0.76      1084\n",
      "      I-LOC       0.61      0.60      0.61       325\n",
      "     B-MISC       0.65      0.53      0.59       339\n",
      "     I-MISC       0.58      0.50      0.54       557\n",
      "      B-ORG       0.78      0.83      0.80      1400\n",
      "      I-ORG       0.79      0.76      0.78      1104\n",
      "      B-PER       0.86      0.81      0.84       735\n",
      "      I-PER       0.92      0.89      0.91       634\n",
      "\n",
      "avg / total       0.77      0.75      0.76      6178\n",
      "\n",
      "c1: 1.0e-04 - c2: 1.0e-04 \n",
      " Nb non-zero features : 42006 & Coefficient vector norm 621.1 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.78      0.76      0.77      1084\n",
      "      I-LOC       0.65      0.60      0.62       325\n",
      "     B-MISC       0.65      0.53      0.58       339\n",
      "     I-MISC       0.59      0.55      0.57       557\n",
      "      B-ORG       0.78      0.83      0.80      1400\n",
      "      I-ORG       0.80      0.77      0.78      1104\n",
      "      B-PER       0.87      0.82      0.85       735\n",
      "      I-PER       0.92      0.90      0.91       634\n",
      "\n",
      "avg / total       0.78      0.76      0.77      6178\n",
      "\n",
      "c1: 1.0e-04 - c2: 1.0e-03 \n",
      " Nb non-zero features : 43861 & Coefficient vector norm 421.6 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.79      0.76      0.78      1084\n",
      "      I-LOC       0.68      0.62      0.65       325\n",
      "     B-MISC       0.69      0.53      0.60       339\n",
      "     I-MISC       0.62      0.54      0.57       557\n",
      "      B-ORG       0.79      0.84      0.81      1400\n",
      "      I-ORG       0.82      0.78      0.80      1104\n",
      "      B-PER       0.86      0.85      0.86       735\n",
      "      I-PER       0.92      0.93      0.92       634\n",
      "\n",
      "avg / total       0.79      0.77      0.78      6178\n",
      "\n",
      "c1: 1.0e-04 - c2: 1.0e-02 \n",
      " Nb non-zero features : 52352 & Coefficient vector norm 267.5 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.80      0.77      0.78      1084\n",
      "      I-LOC       0.69      0.62      0.65       325\n",
      "     B-MISC       0.72      0.54      0.62       339\n",
      "     I-MISC       0.66      0.57      0.61       557\n",
      "      B-ORG       0.80      0.84      0.82      1400\n",
      "      I-ORG       0.84      0.80      0.82      1104\n",
      "      B-PER       0.85      0.87      0.86       735\n",
      "      I-PER       0.91      0.93      0.92       634\n",
      "\n",
      "avg / total       0.80      0.78      0.79      6178\n",
      "\n",
      "c1: 1.0e-04 - c2: 1.0e-01 \n",
      " Nb non-zero features : 69876 & Coefficient vector norm 149.6 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.81      0.78      0.79      1084\n",
      "      I-LOC       0.71      0.62      0.67       325\n",
      "     B-MISC       0.72      0.55      0.63       339\n",
      "     I-MISC       0.70      0.57      0.63       557\n",
      "      B-ORG       0.81      0.84      0.82      1400\n",
      "      I-ORG       0.85      0.81      0.83      1104\n",
      "      B-PER       0.85      0.88      0.86       735\n",
      "      I-PER       0.89      0.94      0.92       634\n",
      "\n",
      "avg / total       0.81      0.79      0.80      6178\n",
      "\n",
      "c1: 1.0e-04 - c2: 1.0e+00 \n",
      " Nb non-zero features : 85206 & Coefficient vector norm 63.7 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.79      0.77      0.78      1084\n",
      "      I-LOC       0.67      0.62      0.64       325\n",
      "     B-MISC       0.72      0.49      0.58       339\n",
      "     I-MISC       0.72      0.54      0.62       557\n",
      "      B-ORG       0.80      0.82      0.81      1400\n",
      "      I-ORG       0.82      0.81      0.81      1104\n",
      "      B-PER       0.83      0.88      0.85       735\n",
      "      I-PER       0.88      0.94      0.91       634\n",
      "\n",
      "avg / total       0.79      0.77      0.78      6178\n",
      "\n",
      "c1: 1.0e-04 - c2: 1.0e+01 \n",
      " Nb non-zero features : 92398 & Coefficient vector norm 24.2 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.71      0.69      0.70      1084\n",
      "      I-LOC       0.57      0.40      0.47       325\n",
      "     B-MISC       0.61      0.28      0.38       339\n",
      "     I-MISC       0.52      0.39      0.45       557\n",
      "      B-ORG       0.75      0.77      0.76      1400\n",
      "      I-ORG       0.74      0.78      0.76      1104\n",
      "      B-PER       0.76      0.83      0.79       735\n",
      "      I-PER       0.82      0.92      0.87       634\n",
      "\n",
      "avg / total       0.71      0.70      0.70      6178\n",
      "\n",
      "c1: 1.0e-03 - c2: 0.0e+00 \n",
      " Nb non-zero features : 31381 & Coefficient vector norm 644.8 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.78      0.77      0.78      1084\n",
      "      I-LOC       0.63      0.60      0.62       325\n",
      "     B-MISC       0.63      0.51      0.56       339\n",
      "     I-MISC       0.63      0.52      0.57       557\n",
      "      B-ORG       0.78      0.83      0.80      1400\n",
      "      I-ORG       0.80      0.75      0.78      1104\n",
      "      B-PER       0.86      0.83      0.84       735\n",
      "      I-PER       0.92      0.91      0.91       634\n",
      "\n",
      "avg / total       0.78      0.75      0.76      6178\n",
      "\n",
      "c1: 1.0e-03 - c2: 1.0e-04 \n",
      " Nb non-zero features : 33335 & Coefficient vector norm 572.4 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.78      0.76      0.77      1084\n",
      "      I-LOC       0.64      0.61      0.63       325\n",
      "     B-MISC       0.67      0.54      0.60       339\n",
      "     I-MISC       0.61      0.54      0.57       557\n",
      "      B-ORG       0.78      0.83      0.81      1400\n",
      "      I-ORG       0.81      0.78      0.79      1104\n",
      "      B-PER       0.85      0.82      0.84       735\n",
      "      I-PER       0.90      0.90      0.90       634\n",
      "\n",
      "avg / total       0.78      0.76      0.77      6178\n",
      "\n",
      "c1: 1.0e-03 - c2: 1.0e-03 \n",
      " Nb non-zero features : 36367 & Coefficient vector norm 412.6 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.80      0.78      0.79      1084\n",
      "      I-LOC       0.69      0.66      0.68       325\n",
      "     B-MISC       0.69      0.55      0.61       339\n",
      "     I-MISC       0.62      0.56      0.59       557\n",
      "      B-ORG       0.80      0.83      0.82      1400\n",
      "      I-ORG       0.83      0.77      0.80      1104\n",
      "      B-PER       0.85      0.86      0.86       735\n",
      "      I-PER       0.91      0.92      0.92       634\n",
      "\n",
      "avg / total       0.80      0.78      0.78      6178\n",
      "\n",
      "c1: 1.0e-03 - c2: 1.0e-02 \n",
      " Nb non-zero features : 41931 & Coefficient vector norm 267.0 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.81      0.77      0.79      1084\n",
      "      I-LOC       0.70      0.62      0.66       325\n",
      "     B-MISC       0.71      0.54      0.61       339\n",
      "     I-MISC       0.66      0.58      0.61       557\n",
      "      B-ORG       0.80      0.84      0.82      1400\n",
      "      I-ORG       0.84      0.80      0.82      1104\n",
      "      B-PER       0.85      0.87      0.86       735\n",
      "      I-PER       0.91      0.94      0.92       634\n",
      "\n",
      "avg / total       0.80      0.78      0.79      6178\n",
      "\n",
      "c1: 1.0e-03 - c2: 1.0e-01 \n",
      " Nb non-zero features : 53265 & Coefficient vector norm 149.4 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.81      0.79      0.80      1084\n",
      "      I-LOC       0.71      0.63      0.67       325\n",
      "     B-MISC       0.72      0.55      0.62       339\n",
      "     I-MISC       0.69      0.62      0.65       557\n",
      "      B-ORG       0.81      0.84      0.82      1400\n",
      "      I-ORG       0.85      0.80      0.83      1104\n",
      "      B-PER       0.85      0.88      0.87       735\n",
      "      I-PER       0.90      0.94      0.92       634\n",
      "\n",
      "avg / total       0.81      0.79      0.80      6178\n",
      "\n",
      "c1: 1.0e-03 - c2: 1.0e+00 \n",
      " Nb non-zero features : 68109 & Coefficient vector norm 63.7 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.78      0.77      0.78      1084\n",
      "      I-LOC       0.64      0.63      0.63       325\n",
      "     B-MISC       0.72      0.49      0.58       339\n",
      "     I-MISC       0.71      0.54      0.62       557\n",
      "      B-ORG       0.80      0.81      0.80      1400\n",
      "      I-ORG       0.83      0.80      0.81      1104\n",
      "      B-PER       0.83      0.88      0.85       735\n",
      "      I-PER       0.89      0.94      0.91       634\n",
      "\n",
      "avg / total       0.79      0.77      0.78      6178\n",
      "\n",
      "c1: 1.0e-03 - c2: 1.0e+01 \n",
      " Nb non-zero features : 79626 & Coefficient vector norm 24.2 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.71      0.69      0.70      1084\n",
      "      I-LOC       0.58      0.39      0.46       325\n",
      "     B-MISC       0.59      0.27      0.37       339\n",
      "     I-MISC       0.52      0.40      0.46       557\n",
      "      B-ORG       0.75      0.77      0.76      1400\n",
      "      I-ORG       0.74      0.79      0.76      1104\n",
      "      B-PER       0.76      0.83      0.79       735\n",
      "      I-PER       0.82      0.92      0.87       634\n",
      "\n",
      "avg / total       0.71      0.70      0.70      6178\n",
      "\n",
      "c1: 1.0e-02 - c2: 0.0e+00 \n",
      " Nb non-zero features : 25586 & Coefficient vector norm 445.5 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.79      0.77      0.78      1084\n",
      "      I-LOC       0.67      0.64      0.66       325\n",
      "     B-MISC       0.69      0.53      0.60       339\n",
      "     I-MISC       0.66      0.53      0.59       557\n",
      "      B-ORG       0.79      0.83      0.81      1400\n",
      "      I-ORG       0.82      0.77      0.79      1104\n",
      "      B-PER       0.85      0.85      0.85       735\n",
      "      I-PER       0.90      0.93      0.91       634\n",
      "\n",
      "avg / total       0.79      0.77      0.78      6178\n",
      "\n",
      "c1: 1.0e-02 - c2: 1.0e-04 \n",
      " Nb non-zero features : 25407 & Coefficient vector norm 428.9 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.79      0.77      0.78      1084\n",
      "      I-LOC       0.69      0.65      0.67       325\n",
      "     B-MISC       0.69      0.56      0.62       339\n",
      "     I-MISC       0.63      0.58      0.60       557\n",
      "      B-ORG       0.80      0.83      0.81      1400\n",
      "      I-ORG       0.83      0.78      0.81      1104\n",
      "      B-PER       0.86      0.85      0.85       735\n",
      "      I-PER       0.91      0.93      0.92       634\n",
      "\n",
      "avg / total       0.79      0.78      0.78      6178\n",
      "\n",
      "c1: 1.0e-02 - c2: 1.0e-03 \n",
      " Nb non-zero features : 27246 & Coefficient vector norm 369.4 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.80      0.77      0.78      1084\n",
      "      I-LOC       0.70      0.64      0.67       325\n",
      "     B-MISC       0.70      0.54      0.61       339\n",
      "     I-MISC       0.63      0.56      0.59       557\n",
      "      B-ORG       0.80      0.83      0.81      1400\n",
      "      I-ORG       0.82      0.77      0.80      1104\n",
      "      B-PER       0.85      0.86      0.86       735\n",
      "      I-PER       0.90      0.94      0.92       634\n",
      "\n",
      "avg / total       0.79      0.77      0.78      6178\n",
      "\n",
      "c1: 1.0e-02 - c2: 1.0e-02 \n",
      " Nb non-zero features : 32459 & Coefficient vector norm 257.0 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.81      0.78      0.79      1084\n",
      "      I-LOC       0.70      0.63      0.66       325\n",
      "     B-MISC       0.70      0.54      0.61       339\n",
      "     I-MISC       0.66      0.57      0.61       557\n",
      "      B-ORG       0.80      0.83      0.82      1400\n",
      "      I-ORG       0.84      0.78      0.81      1104\n",
      "      B-PER       0.84      0.87      0.86       735\n",
      "      I-PER       0.91      0.94      0.92       634\n",
      "\n",
      "avg / total       0.80      0.78      0.79      6178\n",
      "\n",
      "c1: 1.0e-02 - c2: 1.0e-01 \n",
      " Nb non-zero features : 40291 & Coefficient vector norm 147.6 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.81      0.78      0.80      1084\n",
      "      I-LOC       0.71      0.63      0.66       325\n",
      "     B-MISC       0.72      0.55      0.63       339\n",
      "     I-MISC       0.72      0.59      0.64       557\n",
      "      B-ORG       0.81      0.84      0.82      1400\n",
      "      I-ORG       0.85      0.81      0.83      1104\n",
      "      B-PER       0.85      0.88      0.86       735\n",
      "      I-PER       0.89      0.94      0.91       634\n",
      "\n",
      "avg / total       0.81      0.79      0.80      6178\n",
      "\n",
      "c1: 1.0e-02 - c2: 1.0e+00 \n",
      " Nb non-zero features : 49318 & Coefficient vector norm 63.2 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.78      0.76      0.77      1084\n",
      "      I-LOC       0.64      0.62      0.63       325\n",
      "     B-MISC       0.72      0.49      0.58       339\n",
      "     I-MISC       0.69      0.54      0.61       557\n",
      "      B-ORG       0.80      0.81      0.81      1400\n",
      "      I-ORG       0.84      0.80      0.82      1104\n",
      "      B-PER       0.83      0.88      0.85       735\n",
      "      I-PER       0.88      0.94      0.91       634\n",
      "\n",
      "avg / total       0.79      0.77      0.78      6178\n",
      "\n",
      "c1: 1.0e-02 - c2: 1.0e+01 \n",
      " Nb non-zero features : 56929 & Coefficient vector norm 24.1 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.71      0.69      0.70      1084\n",
      "      I-LOC       0.57      0.40      0.47       325\n",
      "     B-MISC       0.58      0.27      0.37       339\n",
      "     I-MISC       0.52      0.39      0.45       557\n",
      "      B-ORG       0.75      0.77      0.76      1400\n",
      "      I-ORG       0.75      0.78      0.77      1104\n",
      "      B-PER       0.76      0.83      0.80       735\n",
      "      I-PER       0.82      0.93      0.87       634\n",
      "\n",
      "avg / total       0.71      0.70      0.70      6178\n",
      "\n",
      "c1: 1.0e-01 - c2: 0.0e+00 \n",
      " Nb non-zero features : 18207 & Coefficient vector norm 262.6 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.80      0.79      0.79      1084\n",
      "      I-LOC       0.64      0.68      0.66       325\n",
      "     B-MISC       0.71      0.56      0.63       339\n",
      "     I-MISC       0.67      0.59      0.63       557\n",
      "      B-ORG       0.81      0.83      0.82      1400\n",
      "      I-ORG       0.86      0.76      0.80      1104\n",
      "      B-PER       0.85      0.88      0.86       735\n",
      "      I-PER       0.90      0.94      0.92       634\n",
      "\n",
      "avg / total       0.80      0.78      0.79      6178\n",
      "\n",
      "c1: 1.0e-01 - c2: 1.0e-04 \n",
      " Nb non-zero features : 18264 & Coefficient vector norm 263.4 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.80      0.78      0.79      1084\n",
      "      I-LOC       0.66      0.65      0.66       325\n",
      "     B-MISC       0.69      0.55      0.61       339\n",
      "     I-MISC       0.69      0.60      0.64       557\n",
      "      B-ORG       0.80      0.83      0.82      1400\n",
      "      I-ORG       0.85      0.76      0.80      1104\n",
      "      B-PER       0.85      0.88      0.87       735\n",
      "      I-PER       0.90      0.94      0.92       634\n",
      "\n",
      "avg / total       0.80      0.78      0.79      6178\n",
      "\n",
      "c1: 1.0e-01 - c2: 1.0e-03 \n",
      " Nb non-zero features : 18432 & Coefficient vector norm 255.9 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.80      0.78      0.79      1084\n",
      "      I-LOC       0.69      0.66      0.68       325\n",
      "     B-MISC       0.69      0.55      0.61       339\n",
      "     I-MISC       0.67      0.60      0.63       557\n",
      "      B-ORG       0.81      0.83      0.82      1400\n",
      "      I-ORG       0.85      0.78      0.81      1104\n",
      "      B-PER       0.85      0.88      0.87       735\n",
      "      I-PER       0.90      0.94      0.92       634\n",
      "\n",
      "avg / total       0.80      0.79      0.79      6178\n",
      "\n",
      "c1: 1.0e-01 - c2: 1.0e-02 \n",
      " Nb non-zero features : 20377 & Coefficient vector norm 216.3 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.80      0.79      0.80      1084\n",
      "      I-LOC       0.68      0.68      0.68       325\n",
      "     B-MISC       0.71      0.56      0.63       339\n",
      "     I-MISC       0.68      0.59      0.63       557\n",
      "      B-ORG       0.81      0.83      0.82      1400\n",
      "      I-ORG       0.86      0.77      0.81      1104\n",
      "      B-PER       0.84      0.88      0.86       735\n",
      "      I-PER       0.90      0.94      0.92       634\n",
      "\n",
      "avg / total       0.81      0.79      0.79      6178\n",
      "\n",
      "c1: 1.0e-01 - c2: 1.0e-01 \n",
      " Nb non-zero features : 26846 & Coefficient vector norm 135.6 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.81      0.79      0.80      1084\n",
      "      I-LOC       0.67      0.64      0.66       325\n",
      "     B-MISC       0.72      0.56      0.63       339\n",
      "     I-MISC       0.72      0.60      0.65       557\n",
      "      B-ORG       0.82      0.83      0.83      1400\n",
      "      I-ORG       0.86      0.79      0.82      1104\n",
      "      B-PER       0.84      0.89      0.86       735\n",
      "      I-PER       0.89      0.94      0.91       634\n",
      "\n",
      "avg / total       0.81      0.79      0.80      6178\n",
      "\n",
      "c1: 1.0e-01 - c2: 1.0e+00 \n",
      " Nb non-zero features : 34325 & Coefficient vector norm 60.6 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.78      0.76      0.77      1084\n",
      "      I-LOC       0.65      0.62      0.64       325\n",
      "     B-MISC       0.71      0.49      0.58       339\n",
      "     I-MISC       0.70      0.55      0.61       557\n",
      "      B-ORG       0.80      0.81      0.80      1400\n",
      "      I-ORG       0.82      0.80      0.81      1104\n",
      "      B-PER       0.83      0.88      0.85       735\n",
      "      I-PER       0.88      0.94      0.91       634\n",
      "\n",
      "avg / total       0.79      0.77      0.78      6178\n",
      "\n",
      "c1: 1.0e-01 - c2: 1.0e+01 \n",
      " Nb non-zero features : 39349 & Coefficient vector norm 24.0 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.71      0.69      0.70      1084\n",
      "      I-LOC       0.56      0.39      0.46       325\n",
      "     B-MISC       0.60      0.27      0.37       339\n",
      "     I-MISC       0.53      0.39      0.45       557\n",
      "      B-ORG       0.75      0.77      0.76      1400\n",
      "      I-ORG       0.74      0.79      0.76      1104\n",
      "      B-PER       0.76      0.83      0.79       735\n",
      "      I-PER       0.82      0.92      0.87       634\n",
      "\n",
      "avg / total       0.71      0.70      0.70      6178\n",
      "\n",
      "c1: 1.0e+00 - c2: 0.0e+00 \n",
      " Nb non-zero features : 6515 & Coefficient vector norm 96.1 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.78      0.75      0.76      1084\n",
      "      I-LOC       0.61      0.62      0.62       325\n",
      "     B-MISC       0.69      0.51      0.59       339\n",
      "     I-MISC       0.66      0.58      0.62       557\n",
      "      B-ORG       0.80      0.80      0.80      1400\n",
      "      I-ORG       0.83      0.78      0.80      1104\n",
      "      B-PER       0.81      0.88      0.85       735\n",
      "      I-PER       0.86      0.93      0.89       634\n",
      "\n",
      "avg / total       0.78      0.77      0.77      6178\n",
      "\n",
      "c1: 1.0e+00 - c2: 1.0e-04 \n",
      " Nb non-zero features : 6520 & Coefficient vector norm 95.9 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.78      0.76      0.77      1084\n",
      "      I-LOC       0.62      0.63      0.63       325\n",
      "     B-MISC       0.69      0.50      0.58       339\n",
      "     I-MISC       0.66      0.56      0.61       557\n",
      "      B-ORG       0.80      0.81      0.80      1400\n",
      "      I-ORG       0.83      0.79      0.81      1104\n",
      "      B-PER       0.82      0.88      0.85       735\n",
      "      I-PER       0.87      0.94      0.90       634\n",
      "\n",
      "avg / total       0.78      0.77      0.78      6178\n",
      "\n",
      "c1: 1.0e+00 - c2: 1.0e-03 \n",
      " Nb non-zero features : 6351 & Coefficient vector norm 96.1 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.78      0.76      0.77      1084\n",
      "      I-LOC       0.63      0.62      0.63       325\n",
      "     B-MISC       0.68      0.51      0.58       339\n",
      "     I-MISC       0.66      0.58      0.62       557\n",
      "      B-ORG       0.80      0.80      0.80      1400\n",
      "      I-ORG       0.83      0.78      0.81      1104\n",
      "      B-PER       0.82      0.88      0.85       735\n",
      "      I-PER       0.87      0.93      0.90       634\n",
      "\n",
      "avg / total       0.78      0.77      0.78      6178\n",
      "\n",
      "c1: 1.0e+00 - c2: 1.0e-02 \n",
      " Nb non-zero features : 6571 & Coefficient vector norm 93.4 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.78      0.76      0.77      1084\n",
      "      I-LOC       0.62      0.63      0.63       325\n",
      "     B-MISC       0.67      0.51      0.58       339\n",
      "     I-MISC       0.63      0.58      0.60       557\n",
      "      B-ORG       0.80      0.80      0.80      1400\n",
      "      I-ORG       0.83      0.77      0.80      1104\n",
      "      B-PER       0.82      0.88      0.85       735\n",
      "      I-PER       0.87      0.93      0.90       634\n",
      "\n",
      "avg / total       0.78      0.77      0.77      6178\n",
      "\n",
      "c1: 1.0e+00 - c2: 1.0e-01 \n",
      " Nb non-zero features : 7214 & Coefficient vector norm 78.0 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.78      0.76      0.77      1084\n",
      "      I-LOC       0.64      0.62      0.63       325\n",
      "     B-MISC       0.69      0.50      0.58       339\n",
      "     I-MISC       0.64      0.55      0.59       557\n",
      "      B-ORG       0.80      0.80      0.80      1400\n",
      "      I-ORG       0.83      0.78      0.80      1104\n",
      "      B-PER       0.81      0.88      0.85       735\n",
      "      I-PER       0.86      0.93      0.90       634\n",
      "\n",
      "avg / total       0.78      0.76      0.77      6178\n",
      "\n",
      "c1: 1.0e+00 - c2: 1.0e+00 \n",
      " Nb non-zero features : 9131 & Coefficient vector norm 45.8 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.76      0.74      0.75      1084\n",
      "      I-LOC       0.63      0.55      0.59       325\n",
      "     B-MISC       0.67      0.42      0.51       339\n",
      "     I-MISC       0.59      0.52      0.55       557\n",
      "      B-ORG       0.78      0.79      0.78      1400\n",
      "      I-ORG       0.80      0.79      0.79      1104\n",
      "      B-PER       0.79      0.88      0.83       735\n",
      "      I-PER       0.85      0.94      0.89       634\n",
      "\n",
      "avg / total       0.76      0.75      0.75      6178\n",
      "\n",
      "c1: 1.0e+00 - c2: 1.0e+01 \n",
      " Nb non-zero features : 11752 & Coefficient vector norm 22.4 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.71      0.69      0.70      1084\n",
      "      I-LOC       0.56      0.37      0.44       325\n",
      "     B-MISC       0.57      0.25      0.35       339\n",
      "     I-MISC       0.50      0.38      0.43       557\n",
      "      B-ORG       0.74      0.76      0.75      1400\n",
      "      I-ORG       0.73      0.79      0.76      1104\n",
      "      B-PER       0.75      0.82      0.78       735\n",
      "      I-PER       0.82      0.92      0.86       634\n",
      "\n",
      "avg / total       0.70      0.69      0.69      6178\n",
      "\n",
      "c1: 1.0e+01 - c2: 0.0e+00 \n",
      " Nb non-zero features : 1160 & Coefficient vector norm 32.2 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.68      0.68      0.68      1084\n",
      "      I-LOC       0.46      0.36      0.40       325\n",
      "     B-MISC       0.52      0.23      0.32       339\n",
      "     I-MISC       0.47      0.36      0.41       557\n",
      "      B-ORG       0.73      0.74      0.74      1400\n",
      "      I-ORG       0.74      0.76      0.75      1104\n",
      "      B-PER       0.72      0.80      0.76       735\n",
      "      I-PER       0.78      0.91      0.84       634\n",
      "\n",
      "avg / total       0.68      0.68      0.67      6178\n",
      "\n",
      "c1: 1.0e+01 - c2: 1.0e-04 \n",
      " Nb non-zero features : 1157 & Coefficient vector norm 32.1 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.68      0.68      0.68      1084\n",
      "      I-LOC       0.46      0.36      0.40       325\n",
      "     B-MISC       0.53      0.23      0.32       339\n",
      "     I-MISC       0.47      0.36      0.41       557\n",
      "      B-ORG       0.74      0.75      0.74      1400\n",
      "      I-ORG       0.74      0.77      0.76      1104\n",
      "      B-PER       0.72      0.80      0.76       735\n",
      "      I-PER       0.79      0.91      0.84       634\n",
      "\n",
      "avg / total       0.68      0.68      0.67      6178\n",
      "\n",
      "c1: 1.0e+01 - c2: 1.0e-03 \n",
      " Nb non-zero features : 1149 & Coefficient vector norm 32.2 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.68      0.68      0.68      1084\n",
      "      I-LOC       0.46      0.36      0.41       325\n",
      "     B-MISC       0.52      0.22      0.31       339\n",
      "     I-MISC       0.46      0.36      0.40       557\n",
      "      B-ORG       0.73      0.74      0.74      1400\n",
      "      I-ORG       0.74      0.76      0.75      1104\n",
      "      B-PER       0.72      0.80      0.76       735\n",
      "      I-PER       0.79      0.91      0.84       634\n",
      "\n",
      "avg / total       0.68      0.68      0.67      6178\n",
      "\n",
      "c1: 1.0e+01 - c2: 1.0e-02 \n",
      " Nb non-zero features : 1164 & Coefficient vector norm 31.8 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.68      0.68      0.68      1084\n",
      "      I-LOC       0.46      0.35      0.40       325\n",
      "     B-MISC       0.53      0.22      0.31       339\n",
      "     I-MISC       0.47      0.36      0.41       557\n",
      "      B-ORG       0.74      0.74      0.74      1400\n",
      "      I-ORG       0.74      0.76      0.75      1104\n",
      "      B-PER       0.72      0.80      0.76       735\n",
      "      I-PER       0.78      0.91      0.84       634\n",
      "\n",
      "avg / total       0.68      0.68      0.67      6178\n",
      "\n",
      "c1: 1.0e+01 - c2: 1.0e-01 \n",
      " Nb non-zero features : 1115 & Coefficient vector norm 30.9 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.69      0.67      0.68      1084\n",
      "      I-LOC       0.50      0.35      0.41       325\n",
      "     B-MISC       0.51      0.22      0.31       339\n",
      "     I-MISC       0.49      0.35      0.41       557\n",
      "      B-ORG       0.73      0.75      0.74      1400\n",
      "      I-ORG       0.72      0.76      0.74      1104\n",
      "      B-PER       0.71      0.80      0.76       735\n",
      "      I-PER       0.77      0.91      0.84       634\n",
      "\n",
      "avg / total       0.68      0.67      0.67      6178\n",
      "\n",
      "c1: 1.0e+01 - c2: 1.0e+00 \n",
      " Nb non-zero features : 1213 & Coefficient vector norm 27.5 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.69      0.67      0.68      1084\n",
      "      I-LOC       0.47      0.33      0.39       325\n",
      "     B-MISC       0.53      0.22      0.31       339\n",
      "     I-MISC       0.54      0.34      0.42       557\n",
      "      B-ORG       0.73      0.75      0.74      1400\n",
      "      I-ORG       0.72      0.77      0.74      1104\n",
      "      B-PER       0.71      0.81      0.76       735\n",
      "      I-PER       0.77      0.92      0.84       634\n",
      "\n",
      "avg / total       0.68      0.67      0.67      6178\n",
      "\n",
      "c1: 1.0e+01 - c2: 1.0e+01 \n",
      " Nb non-zero features : 1477 & Coefficient vector norm 19.0 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.68      0.65      0.66      1084\n",
      "      I-LOC       0.47      0.26      0.34       325\n",
      "     B-MISC       0.53      0.15      0.24       339\n",
      "     I-MISC       0.57      0.26      0.36       557\n",
      "      B-ORG       0.70      0.74      0.72      1400\n",
      "      I-ORG       0.67      0.77      0.72      1104\n",
      "      B-PER       0.70      0.79      0.75       735\n",
      "      I-PER       0.77      0.92      0.84       634\n",
      "\n",
      "avg / total       0.67      0.65      0.65      6178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "    \n",
    "c1_values = [0, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
    "c2_values = [0, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
    "\n",
    "#c1_values = [0, 1e-4, 1e0, 1e1]\n",
    "#c2_values = [0, 1e-4, 1e0, 1e1]\n",
    "\n",
    "# loop over the regularization values\n",
    "for c1, c2 in product(c1_values, c2_values):\n",
    "    trainer.set_params({\n",
    "        'c1': c1,   # coefficient for L1 penalty\n",
    "        'c2': c2,  # coefficient for L2 penalty\n",
    "        'max_iterations': 150,  \n",
    "        # include transitions that are possible, but not observed\n",
    "        'feature.possible_transitions': True\n",
    "    })\n",
    "\n",
    "    #model_name = 'conll2002-esp.crfsuite_%.1e_%.1e' % (c1, c2)\n",
    "    model_name = 'conll2002-esp.crfsuite_tmp'\n",
    "    trainer.train(model_name)\n",
    "\n",
    "    log = trainer.logparser.last_iteration\n",
    "    nb_nonzero = log['active_features']\n",
    "    feature_norm = log['feature_norm']\n",
    "\n",
    "    tagger = pycrfsuite.Tagger()\n",
    "    tagger.open(model_name)\n",
    "\n",
    "    y_pred = [tagger.tag(xseq) for xseq in X_test]\n",
    "\n",
    "    print 'c1: %.1e - c2: %.1e \\n Nb non-zero features : %.0f & Coefficient vector norm %.1f \\n' % (c1, c2, nb_nonzero, feature_norm)\n",
    "    print(bio_classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis :  \n",
    "\n",
    "We note that the behavior of the parameter vector as function of c1 and c2 is as expected. \n",
    "Indeed, the greater c1 the sparser the vector. And the greater c2 the smaller the norm of the vector.\n",
    "These are the classical behaviors corresponding to these kind of regularizations i.e. l1 and l2.  \n",
    "There are several maximum (0.80) for the mean f1-score. All of them with a value of c2 = 1e-01 (and c1=1e-4, 1e-3, 1e-2, 1e-1). Therefore the l2 regularization seems to be most important for this task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi decoding\n",
    "\n",
    "Here, we implement our version of the viterbi algorithm. We use tools from the flexcrf suite.  \n",
    "We obtain the same results than with crfsuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data loaded.\n",
      "\n",
      "converting to flexcrf format...\n",
      "f_xy_desc created.\n",
      "t_xyy_desc created\n",
      "Processing sentence 1/3...\n",
      "Processing sentence 2/3...\n",
      "Processing sentence 3/3...\n",
      "\n",
      "-- With crfsuite:\n",
      "labels:\n",
      "['B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O'] \n",
      "[0 7 1 1 1 1 2 1 1]\n",
      "probability:\t 0.930321\n",
      "-- With flexcrf:\n",
      "labels:\n",
      "[0 7 1 1 1 1 2 1 1]\n",
      "equal predictions:  True\n",
      "probability:\t 0.930321\n",
      "delta:\t 0.000000\n",
      "\n",
      "-- With crfsuite:\n",
      "labels:\n",
      "['O'] \n",
      "[1]\n",
      "probability:\t 0.999996\n",
      "-- With flexcrf:\n",
      "labels:\n",
      "[1]\n",
      "equal predictions:  True\n",
      "probability:\t 0.999996\n",
      "delta:\t 0.000000\n",
      "\n",
      "-- With crfsuite:\n",
      "labels:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O'] \n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 5 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 3 4 4 1 1 1 1 1 0 7 1]\n",
      "probability:\t 0.437425\n",
      "-- With flexcrf:\n",
      "labels:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 5 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 3 4 4 1 1 1 1 1 0 7 1]\n",
      "equal predictions:  True\n",
      "probability:\t 0.437425\n",
      "delta:\t 0.000000\n"
     ]
    }
   ],
   "source": [
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "from flexcrf_tp.models.linear_chain import (_feat_fun_values,\n",
    "                                            _compute_all_potentials,\n",
    "                                            _forward_score,\n",
    "                                            _backward_score,\n",
    "                                            _partition_fun_value,\n",
    "                                            _posterior_score)\n",
    "\n",
    "from flexcrf_tp.crfsuite2flexcrf import convert_data_to_flexcrf\n",
    "\n",
    "# -- Define vitrebi_decoder here:\n",
    "\n",
    "def viterbi_decoder(m_xy, n=None, log_version=True):\n",
    "    \"\"\"\n",
    "    Performs MAP inference, determining $y = \\argmax_y P(y|x)$, using the\n",
    "    Viterbi algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m_xy : ndarray, shape (n_obs, n_labels, n_labels)\n",
    "        Values of log-potentials ($\\log M_i(y_{i-1}, y_i, x)$)\n",
    "        computed based on feature functions f_xy and/or user-defined potentials\n",
    "        `psi_xy`. At t=0, m_xy[0, 0, :] contains values of $\\log M_1(y_0, y_1)$\n",
    "        with $y_0$ the fixed initial state.\n",
    "\n",
    "    n : integer, default=None\n",
    "        Time position up to which to decode the optimal sequence; if not\n",
    "        specified (default) the score is computed for the whole sequence.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : ndarray, shape (n_obs,)\n",
    "        Predicted optimal sequence of labels.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    n_seq, n_label, _ = m_xy.shape \n",
    "    record_argmax = np.zeros((n_seq - 1, n_label), dtype=int)\n",
    "    y_pred = np.zeros(n_seq, dtype=int)\n",
    "\n",
    "    # init\n",
    "    score_max = m_xy[0, 0, :].copy()\n",
    "\n",
    "    # loop - computing the new scores along the sequence and recording the paths\n",
    "    for i_seq in range(n_seq - 1):\n",
    "        tmp = score_max.reshape(-1, 1) + m_xy[i_seq + 1] # using broadcasting from numpy to make the code efficient\n",
    "        record_argmax[i_seq] = np.argmax(tmp, axis=0)\n",
    "        score_max = tmp[record_argmax[i_seq], np.arange(n_label)]\n",
    "\n",
    "    # back-tracking - find the path leading to the best sequence\n",
    "    y_pred[-1] = np.argmax(score_max)\n",
    "    for i_seq in np.arange(n_seq - 1)[::-1]:\n",
    "        y_pred[i_seq] = record_argmax[i_seq, y_pred[i_seq + 1]]\n",
    "\n",
    "    return y_pred, score_max[y_pred[-1]]\n",
    "\n",
    "\n",
    "# -- Load data and crfsuite model and convert them-------------------------\n",
    "\n",
    "RECREATE = True  # set to True to recreate flexcrf data with new model\n",
    "\n",
    "CRFSUITE_MODEL_FILE = '../conll2002/conll2002-esp.crfsuite'\n",
    "CRFSUITE_TEST_DATA_FILE = '../conll2002/conll2002-esp_crfsuite-test-data.dump'\n",
    "FLEXCRF_TEST_DATA_FILE = '../conll2002/conll2002-esp_flexcrf-test-data.dump'\n",
    "\n",
    "# crfsuite model\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open(CRFSUITE_MODEL_FILE)\n",
    "model = tagger.info()\n",
    "\n",
    "data = {'X': X_test, 'y': y_test} #pickle.load(open(CRFSUITE_TEST_DATA_FILE))\n",
    "print \"test data loaded.\"\n",
    "\n",
    "if RECREATE:\n",
    "    dataset, thetas = convert_data_to_flexcrf(data, model, n_seq=3)\n",
    "    pickle.dump({'dataset': dataset, 'thetas': thetas}, open(FLEXCRF_TEST_DATA_FILE, 'wb'))\n",
    "else:\n",
    "    dd = pickle.load(open(FLEXCRF_TEST_DATA_FILE))\n",
    "    dataset = dd['dataset']\n",
    "    thetas = dd['thetas']\n",
    "\n",
    "# -- Start classification ------------------------------------------------\n",
    "\n",
    "for seq in range(len(dataset)):\n",
    "    # -- with crfsuite\n",
    "    s_ = tagger.tag(data['X'][seq])\n",
    "    y_ = np.array([int(model.labels[s]) for s in s_])\n",
    "    prob_ = tagger.probability(s_)\n",
    "\n",
    "    print \"\\n-- With crfsuite:\"\n",
    "    print \"labels:\\n\", s_, \"\\n\", y_\n",
    "    print \"probability:\\t %f\" % prob_\n",
    "\n",
    "    # -- with flexcrf\n",
    "    f_xy, y = dataset[seq]\n",
    "    theta = thetas[seq]\n",
    "    \n",
    "    # pre-compute potentials\n",
    "    m_xy, f_m_xy = _compute_all_potentials(f_xy, theta)\n",
    "    \n",
    "    # find the mode \n",
    "    y_pred, s_max = viterbi_decoder(m_xy)\n",
    "    \n",
    "    # compute the forward variables and the partition function\n",
    "    alpha = _forward_score(m_xy, n=None, log_version=True)\n",
    "    Z = np.sum(np.exp(alpha[-1]))\n",
    "    \n",
    "    # posterior probability of the best sequence i.e p(y|x)\n",
    "    prob = np.exp(s_max) / Z\n",
    "\n",
    "    print \"-- With flexcrf:\"\n",
    "    print \"labels:\\n\", y_pred\n",
    "    print \"equal predictions: \", all(y_pred == y_)\n",
    "    print \"probability:\\t %f\" % prob\n",
    "    print \"delta:\\t %f\" % abs(prob - prob_)\n",
    "\n",
    "tagger.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chuncking task\n",
    "\n",
    "We implement a chuncker based on the Conll2000 dataset using the crfsuite.  \n",
    "Our features  are mainly inspired from the tutorial (http://www.chokkan.org/software/crfsuite/tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load Conll2000 dataset\n",
    "train_sents = nltk.corpus.conll2000.iob_sents('train.txt')\n",
    "test_sents = nltk.corpus.conll2000.iob_sents('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature function\n",
    "\n",
    "def word2features(sent, i):\n",
    "    \n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'postag=' + postag,\n",
    "        'postag[:2]=' + postag[:2]\n",
    "    ]\n",
    "    \n",
    "    if i > 1:\n",
    "        word_b_1 = sent[i - 1][0]\n",
    "        postag_b_1 = sent[i - 1][1]\n",
    "        \n",
    "        word_b_2 = sent[i - 2][0]\n",
    "        postag_b_2 = sent[i - 2][1]\n",
    "        \n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word_b_1.lower(),\n",
    "            '-1:word.istitle=%s' % word_b_1.istitle(),\n",
    "            '-1:word.isupper=%s' % word_b_1.isupper(),\n",
    "            '-1:postag=' + postag_b_1,\n",
    "            '-1:postag[:2]=' + postag_b_1[:2],\n",
    "                \n",
    "            # two words before\n",
    "            '-2:word.lower=' + word_b_2.lower(),\n",
    "            '-2:word.istitle=%s' % word_b_2.istitle(),\n",
    "            '-2:word.isupper=%s' % word_b_2.isupper(),\n",
    "            '-2:postag=' + postag_b_2,\n",
    "            '-2:postag[:2]=' + postag_b_2[:2],\n",
    "                \n",
    "            # word sequence\n",
    "            '-1:word.word=' + word_b_1.lower() + word.lower(),\n",
    "                \n",
    "            '-2:word.word=' + word_b_2.lower() + word_b_1.lower(),\n",
    "            '-2:word.word.word=' + word_b_2.lower() + word_b_1.lower() + word.lower(),\n",
    "                \n",
    "            # tag sequence    \n",
    "            '-1:postag.postag=' + postag_b_1 + postag,\n",
    "                \n",
    "            '-2:postag.postag=' + postag_b_2 + postag_b_1,\n",
    "            '-2:postag.postag.postag=' + postag_b_2 + postag_b_1 + postag\n",
    "                \n",
    "        ])\n",
    "        \n",
    "    if i < len(sent) - 2:\n",
    "        word_a_1 = sent[i + 1][0]\n",
    "        postag_a_1 = sent[i + 1][1]\n",
    "        word_a_2 = sent[i + 2][0]\n",
    "        postag_a_2 = sent[i + 2][1]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word_a_1.lower(),\n",
    "            '+1:word.istitle=%s' % word_a_1.istitle(),\n",
    "            '+1:word.isupper=%s' % word_a_1.isupper(),\n",
    "            '+1:postag=' + postag_a_1,\n",
    "            '+1:postag[:2]=' + postag_a_1[:2],\n",
    "            \n",
    "            # two words before\n",
    "            '+2:word.lower=' + word_a_2.lower(),\n",
    "            '+2:word.istitle=%s' % word_a_2.istitle(),\n",
    "            '+2:word.isupper=%s' % word_a_2.isupper(),\n",
    "            '+2:postag=' + postag_a_2,\n",
    "            '+2:postag[:2]=' + postag_a_2[:2],\n",
    "                \n",
    "            # word sequence\n",
    "            '+1:word.word=' + word.lower() + word_a_1.lower(),\n",
    "                \n",
    "            '+2:word.word=' + word_a_1.lower() + word_a_2.lower(),\n",
    "            '+2:word.word.word=' + word.lower() + word_a_1.lower() + word_a_2.lower() ,\n",
    "                \n",
    "            # tag sequence    \n",
    "            '+1:postag.postag=' + postag + postag_a_1,\n",
    "                \n",
    "            '+2:postag.postag=' + postag_a_1 + postag_a_2,\n",
    "            '+2:postag.postag.postag=' + postag + postag_a_1 + postag_a_2\n",
    "        ])\n",
    "    \n",
    "    if i > 0 and i < len(sent) - 1:\n",
    "        word_a_1 = sent[i + 1][0]\n",
    "        postag_a_1 = sent[i + 1][1]\n",
    "        word_b_1 = sent[i - 1][0]\n",
    "        postag_b_1 = sent[i - 1][1]\n",
    "        \n",
    "        features.extend([\n",
    "            # word sequence\n",
    "            '+1:word.word='  + word.lower() + word_a_1.lower(),\n",
    "            '-1:word.word='  + word_b_1.lower() + word.lower(),\n",
    "\n",
    "            'word.word.word=' + word_b_1.lower() + word.lower() + word_a_1.lower(), \n",
    "\n",
    "            # tag sequence    \n",
    "            '+1:postag.postag='  + postag + postag_a_1,\n",
    "            '-1:postag.postag=' + postag_b_1 + postag, \n",
    "\n",
    "            'postag.postag.postag=' + postag_b_1 + postag + postag_a_1\n",
    "        ])\n",
    "        \n",
    "    if i==0:\n",
    "        features.append('BOS')\n",
    "        \n",
    "    \n",
    "    if i==len(sent):\n",
    "        features.append('EOS')\n",
    "        \n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "We take the parameters values found in the above task. Normally we should also do some hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "    \n",
    "trainer.set_params({\n",
    "    'c1': 1e-1,   # coefficient for L1 penalty\n",
    "    'c2': 1e-1,  # coefficient for L2 penalty\n",
    "    'max_iterations': 150,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "trainer.train('conll2000-eng.crfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset evaluation\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     B-ADJP       0.81      0.74      0.78       438\n",
      "     I-ADJP       0.79      0.65      0.71       167\n",
      "     B-ADVP       0.85      0.82      0.83       866\n",
      "     I-ADVP       0.64      0.58      0.61        89\n",
      "    B-CONJP       0.50      0.56      0.53         9\n",
      "    I-CONJP       0.67      0.77      0.71        13\n",
      "     B-INTJ       1.00      1.00      1.00         2\n",
      "      B-LST       0.00      0.00      0.00         5\n",
      "      I-LST       0.00      0.00      0.00         2\n",
      "       B-NP       0.97      0.97      0.97     12422\n",
      "       I-NP       0.97      0.97      0.97     14376\n",
      "       B-PP       0.97      0.98      0.97      4811\n",
      "       I-PP       0.77      0.71      0.74        48\n",
      "      B-PRT       0.82      0.82      0.82       106\n",
      "     B-SBAR       0.89      0.85      0.87       535\n",
      "     I-SBAR       0.17      0.75      0.27         4\n",
      "       B-VP       0.96      0.96      0.96      4658\n",
      "       I-VP       0.95      0.96      0.96      2646\n",
      "\n",
      "avg / total       0.96      0.96      0.96     41197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('conll2000-eng.crfsuite')\n",
    "\n",
    "y_pred = [tagger.tag(xseq) for xseq in X_test]\n",
    "print 'Test dataset evaluation'\n",
    "print(bio_classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset evaluation\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     B-ADJP       1.00      1.00      1.00      2060\n",
      "     I-ADJP       1.00      1.00      1.00       643\n",
      "     B-ADVP       1.00      1.00      1.00      4227\n",
      "     I-ADVP       1.00      1.00      1.00       443\n",
      "    B-CONJP       1.00      1.00      1.00        56\n",
      "    I-CONJP       1.00      1.00      1.00        73\n",
      "     B-INTJ       1.00      0.97      0.98        31\n",
      "     I-INTJ       1.00      1.00      1.00         9\n",
      "      B-LST       1.00      1.00      1.00        10\n",
      "       B-NP       1.00      1.00      1.00     55081\n",
      "       I-NP       1.00      1.00      1.00     63307\n",
      "       B-PP       1.00      1.00      1.00     21281\n",
      "       I-PP       1.00      1.00      1.00       291\n",
      "      B-PRT       1.00      1.00      1.00       556\n",
      "      I-PRT       1.00      1.00      1.00         2\n",
      "     B-SBAR       1.00      1.00      1.00      2207\n",
      "     I-SBAR       1.00      1.00      1.00        70\n",
      "      B-UCP       1.00      1.00      1.00         2\n",
      "      I-UCP       1.00      1.00      1.00         6\n",
      "       B-VP       1.00      1.00      1.00     21467\n",
      "       I-VP       1.00      1.00      1.00     12003\n",
      "\n",
      "avg / total       1.00      1.00      1.00    183825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [tagger.tag(xseq) for xseq in X_train]\n",
    "print 'Train dataset evaluation'\n",
    "print(bio_classification_report(y_train, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Analysis :\n",
    "\n",
    "The mean f1-score (geometric average between the precision and the recall values) for the test set is good (0.96) but we can see that some kind of labels like (B-LST or I-SBAR) have very low score and even zero.\n",
    "The zero score are not really relevant because they have very low support (below 10).  \n",
    "We note that the labels having a support of at least 10000 in the train have very good f1-score.\n",
    "These labels are very frequent and so have a big impact on the loss function.\n",
    "A improvement would be to apply some weights to loss to compensate for the non uniformity of the label distribution. \n",
    "We can reasonably assume that with higher support the labels will have better score.\n",
    "It is possible also that there is some overfitting as the training are very high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
