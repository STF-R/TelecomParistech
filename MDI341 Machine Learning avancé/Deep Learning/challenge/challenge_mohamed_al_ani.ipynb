{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Deep Learning MDI341\n",
    "## Author : Mohamed AL ANI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge rules \n",
    "For face recognition, researchers often build 'templates' (a vector of 128 float values) from high resolution images.  These templates are useful in certain applications where only low resolution images can be acquired and/or there is only low processing power (cpu/gpu) available.\n",
    "\n",
    "\n",
    "The goal of this challenge is to learn a predictive system which predicts templates from **low** resolution images. The predicted templates should be as close as possible to original templates that were built from **high** resolution images.\n",
    "\n",
    "You can use any method you like and any code you like (scikit-learn, tensorflow etc.). However the data provider suggested to see what a Convolutive Neural Network (CNN) could achieve. Again, while we will appreciate if you make it work with a Convolutive Neural Network, but you can use any method you like. However, **if you decide to use a CNN, then Tensorflow is mandatory**.\n",
    "\n",
    "\n",
    "\n",
    "The properties of the dataset are as follows:\n",
    "\n",
    "**Training data:**\n",
    "\n",
    "$\\textbf{X}_{train}$: size 100000 x 2304. Each row of this matrix contains a **low-resolution** image. There are 100000 images where each image is of size 48x48=2304.\n",
    "\n",
    "$\\textbf{Y}_{train}$: size 100000 x 128.  Each row of this matrix contains a template of size 128, which is previously learned from a **high-resolution** image\n",
    "\n",
    "**Validation data:**\n",
    "\n",
    "$\\textbf{X}_{valid}$: size 10000 x 2304. There are 10000 images in this dataset.\n",
    "\n",
    "$\\textbf{Y}_{valid}$: size 10000 x 128.  \n",
    "\n",
    "**Test data:**\n",
    "\n",
    "$\\textbf{X}_{test}$: size 10000 x 2304. There are 10000 images in this dataset similar to the validation set.\n",
    "\n",
    "$\\textbf{Y}_{test}$: size 10000 x 128.  (You do not have access to this matrix)\n",
    "\n",
    "\n",
    "\n",
    "### The goal and the performance criterion\n",
    "\n",
    "The goal is to build a model, which would produce templates for the test data, given $\\textbf{X}_{train}$, $\\textbf{Y}_{train}$, $\\textbf{X}_{valid}$, $\\textbf{Y}_{valid}$, and $\\textbf{X}_{test}$. \n",
    "\n",
    "Let us call the prediction of the model: $\\hat{\\textbf{Y}}_{test}$. The performance criterion is given as follows:\n",
    "\n",
    "$\\text{score} = \\frac1{N}\\sum_{i=1}^{10000} \\sum_{j=1}^{128} \\Bigl(\\textbf{Y}_{test}(i,j) - \\hat{\\textbf{Y}}_{test}(i,j) \\Bigr)^2 $\n",
    "\n",
    "where $N$ denotes the total number of elements in $\\textbf{Y}_{test}$, such that $N=128 \\times 10000$.\n",
    "\n",
    "The lower the score, the better the performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "For this challenge, we will be using a CNN with the help of tensorflow.\n",
    "We tried many different filters and layers, what worked the best for us is : \n",
    "- No activation function\n",
    "- 6 convolution layers (3 3x3 : 2 stride, no padding, and 3 8x8 : 4 strides, no padding)\n",
    "- We used 30000 iterations for the Adam optimizer that appeared to be the optimizer that gives the best results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Inspired by this very good tutorial : https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/02_Convolutional_Neural_Network.ipynb\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from os.path  import join\n",
    "from datetime import timedelta\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was developed using Python 3.5.2 (Anaconda) and TensorFlow version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.0-rc0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total parameters of the cnn \n",
    "Function that calculates the total number of parameters used in the CNN. We make sure not to use more than 50K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_total_param():\n",
    "    ''' Gives the total number of parameters of the CNN (should be < 50k)'''\n",
    "    total_parameters = 0\n",
    "    # iterating over all variables\n",
    "    for variable in tf.trainable_variables():\n",
    "        local_parameters = 1\n",
    "        shape = variable.get_shape()  # getting shape of a variable\n",
    "        for i in shape:\n",
    "            local_parameters *= i.value  # mutiplying dimension values\n",
    "        total_parameters += local_parameters\n",
    "\n",
    "    return total_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Summaries \n",
    "add the variables to Tensors. Can be useful for tensorBoard visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "'''Attach a lot of summaries to a Tensor (for TensorBoard visualization).'''\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and biases initialization\n",
    "Functions for creating new TensorFlow variables in the given shape and initializing them with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New convolution layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates a new convolutional layer in the computational graph for TensorFlow. \n",
    "\n",
    "Input : 4-dim tensor :\n",
    "- Image number.\n",
    "- Y-axis of each image.\n",
    "- X-axis of each image.\n",
    "- Channels of each image.\n",
    "\n",
    "Output : 4-dim tensor :\n",
    "- Image number, same as input.\n",
    "- Y-axis of each image. \n",
    "- X-axis of each image. \n",
    "- Channels produced by the convolutional filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   act=tf.nn.relu):\n",
    "\n",
    "    # Shape of the filter-weights for the convolution.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    with tf.variable_scope(\"conv_layer\"):\n",
    "\n",
    "        with tf.name_scope('weights'):\n",
    "            # Create new weights aka. filters with the given shape.\n",
    "            weights = new_weights(shape=shape)\n",
    "            variable_summaries(weights)\n",
    "        with tf.name_scope('biases'):\n",
    "            # Create new biases, one for each filter.\n",
    "            biases = new_biases(length=num_filters)\n",
    "            variable_summaries(biases)\n",
    "\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            # Create the TensorFlow operation for convolution.\n",
    "            # The padding is set to 'SAME' which means the input image\n",
    "            # is padded with zeroes so the size of the output is the same.\n",
    "            preactivate = tf.nn.conv2d(input=input,\n",
    "                                        filter=weights,\n",
    "                                        strides=[1, 3, 3, 1],\n",
    "                                        padding='SAME') + biases\n",
    "            tf.summary.histogram('pre_activations', preactivate)\n",
    "\n",
    "        # Rectified Linear Unit (ReLU).        \n",
    "        activations = act(preactivate, 'activation')\n",
    "        tf.summary.histogram('activations', activations)\n",
    "\n",
    "    return activations, weights  # act_dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Fully Connected Layer\n",
    "\n",
    "This function creates a new fully-connected layer in the computational graph for TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=False): # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    with tf.name_scope(\"fc_layer\"):\n",
    "        # Create new weights and biases.\n",
    "        weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "        biases = new_biases(length=num_outputs)\n",
    "\n",
    "        # Calculate the layer as the matrix multiplication of\n",
    "        # the input and weights, and then add the bias-values.\n",
    "        layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "        # Use ReLU?\n",
    "        if use_relu:\n",
    "            layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening Layer Dimension\n",
    "\n",
    "From dimension 4 to dimension 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "\n",
    "    with tf.name_scope(\"flatten_layer\"):\n",
    "        # Get the shape of the input layer.\n",
    "        layer_shape = layer.get_shape()\n",
    "\n",
    "        # The shape of the input layer is assumed to be:\n",
    "        # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "        # The number of features is: img_height * img_width * num_channels\n",
    "        # We can use a function from TensorFlow to calculate this.\n",
    "        num_features = layer_shape[1:4].num_elements()\n",
    "\n",
    "        # Reshape the layer to [num_images, num_features].\n",
    "        layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "        # The shape of the flattened layer is now:\n",
    "        # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next batch\n",
    "Return a total of 'num' random samples from the array images and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(num, images, targets):\n",
    "    idx = np.arange(0, images.shape[0])  # get all possible indexes\n",
    "    np.random.shuffle(idx)  # shuffle indexes\n",
    "    idx = idx[0:num]  # use only `num` random indexes\n",
    "    images_shuffle = [images[i] for i in idx]  # get list of `num` random samples\n",
    "    images_shuffle = np.asarray(images_shuffle)  # get back numpy array\n",
    "    targets_shuffle = [targets[i] for i in idx]\n",
    "    targets_shuffle = np.asarray(targets_shuffle)\n",
    "\n",
    "    return images_shuffle, targets_shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restoring a saved model \n",
    "Useful to get all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restore_session(model_path):\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Restore variables from disk.\n",
    "        saver.restore(sess, model_path)\n",
    "        print(\"Model restored from \" + model_path)\n",
    "\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict result using cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_predict(image_data, template_data=None):\n",
    "\n",
    "    # Number of images in the test-set.\n",
    "    num_test = len(valid_template_data)\n",
    "\n",
    "    # Split the test-set into smaller batches of this size.\n",
    "    test_batch_size = 256\n",
    "\n",
    "    # Allocate an array for the predicted templates which\n",
    "    # will be calculated in batches and filled into this array.\n",
    "    temp_pred = np.zeros(shape=(num_test, template_dim), dtype=np.float32)\n",
    "\n",
    "    # Now calculate the predicted templates for the batches.\n",
    "    # We will just iterate through all the batches.\n",
    "\n",
    "    # The starting index for the next batch is denoted i.\n",
    "    i = 0\n",
    "\n",
    "    while i < num_test:\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + test_batch_size, num_test)\n",
    "\n",
    "        # Get the images from the test-set between index i and j.\n",
    "        images = image_data[i:j, :]\n",
    "\n",
    "        # Get the associated labels.\n",
    "        if not template_data is None:\n",
    "            labels = template_data[i:j, :]\n",
    "\n",
    "        # Calculate the prediction using TensorFlow.\n",
    "        temp_pred[i:j, :] = session.run(layer_fc1, feed_dict={x: images})\n",
    "\n",
    "        # Set the start-index for the next batch to the\n",
    "        # end-index of the current batch.\n",
    "        i = j\n",
    "\n",
    "    # Regression score (MSE): Numpy check\n",
    "    sc = np.mean((valid_template_data - temp_pred) ** 2)\n",
    "\n",
    "    if not template_data is None:\n",
    "        # Print the score on the validation set.\n",
    "        msg = \"Score on Test-Set: {0:.4%}\"\n",
    "        print(msg.format(sc))\n",
    "\n",
    "    return temp_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing layers \n",
    "We use just 2 types of convolutional layers : \n",
    "\n",
    "- 16 filters of 8 x 8 pixels\n",
    "- 20 filters of 3 x 3 pixels\n",
    "\n",
    "And the number of neurons that we need in the fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional Layer 1.\n",
    "filter_size1 = 8    # Convolution filters are 8 x 8 pixels.\n",
    "num_filters1 = 16   # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 3    # Convolution filters are 3 x 3 pixels.\n",
    "num_filters2 = 20   # There are 20 of these filters.\n",
    "\n",
    "# Fully-connected layer\n",
    "fc_size = 128             # Number of neurons in fully-connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      "- Training-set:\n",
      "\timages=(100000, 2304)\tlabels=(100000, 128)\n",
      "- Validation-set:\n",
      "\timages=(10000, 2304)\tlabels=(10000, 128)\n",
      "- Test-set:\n",
      "\timages=(10000, 2304)\n"
     ]
    }
   ],
   "source": [
    "images_train_fname    = \"data_train.bin\"\n",
    "templates_train_fname = \"fv_train.bin\"\n",
    "\n",
    "images_valid_fname    = \"data_valid.bin\"\n",
    "templates_valid_fname = \"fv_valid.bin\"\n",
    "\n",
    "images_test_fname     = \"data_test.bin\"\n",
    "\n",
    "# number of images\n",
    "num_train_images = 100000\n",
    "num_valid_images = 10000\n",
    "num_test_images  = 10000\n",
    "\n",
    "# We know that images are 48 pixels in each dimension.\n",
    "img_size = 48\n",
    "\n",
    "# size of the images 48*48 pixels in gray levels\n",
    "img_size_flat = img_size * img_size\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Number of colour channels for the images: 1 channel for gray-scale.\n",
    "num_channels = 1\n",
    "\n",
    "# dimension of the templates\n",
    "template_dim = 128\n",
    "\n",
    "# read the training files\n",
    "with open(templates_train_fname, 'rb') as f:\n",
    "    train_template_data = np.fromfile(f, dtype=np.float32, count=num_train_images * template_dim)\n",
    "    train_template_data = train_template_data.reshape(num_train_images, template_dim)\n",
    "\n",
    "with open(images_train_fname, 'rb') as f:\n",
    "    train_image_data = np.fromfile(f, dtype=np.uint8, count=num_train_images * img_size_flat).astype(np.float32)\n",
    "    train_image_data = train_image_data.reshape(num_train_images, img_size_flat)\n",
    "\n",
    "# read the validation files\n",
    "with open(templates_valid_fname, 'rb') as f:\n",
    "    valid_template_data = np.fromfile(f, dtype=np.float32, count=num_valid_images * template_dim)\n",
    "    valid_template_data = valid_template_data.reshape(num_valid_images, template_dim)\n",
    "\n",
    "with open(images_valid_fname, 'rb') as f:\n",
    "    valid_image_data = np.fromfile(f, dtype=np.uint8, count=num_valid_images * img_size_flat).astype(np.float32)\n",
    "    valid_image_data = valid_image_data.reshape(num_valid_images, img_size_flat)\n",
    "\n",
    "# read the test file\n",
    "with open(images_test_fname, 'rb') as f:\n",
    "    test_image_data = np.fromfile(f, dtype=np.uint8, count=num_test_images * img_size_flat).astype(np.float32)\n",
    "    test_image_data = test_image_data.reshape(num_test_images, img_size_flat)\n",
    "\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\n\\timages={}\\tlabels={}\".format(train_image_data.shape,\n",
    "                                                        train_template_data.shape))\n",
    "print(\"- Validation-set:\\n\\timages={}\\tlabels={}\".format(valid_image_data.shape,\n",
    "                                                   valid_template_data.shape))\n",
    "print(\"- Test-set:\\n\\timages={}\".format(test_image_data.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, template_dim], name='y_true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The convolutional Network\n",
    "We will use 3 convolutional layers (type 1), one pooling layer, 3 other convolutional layers (type 2), one other pooling layer and finally a flatten layer and a fully connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summaries_dir = \"\"\n",
    "\n",
    "# CONVOLUTIONAL LAYER 1\n",
    "\n",
    "layer_conv1, weights_conv1 = \\\n",
    "    new_conv_layer(input=x_image,\n",
    "                   num_input_channels=num_channels,\n",
    "                   filter_size=filter_size1,\n",
    "                   num_filters=num_filters1)\n",
    "                   # use_pooling=True)\n",
    "\n",
    "\n",
    "# CONVOLUTIONAL LAYER 2\n",
    "\n",
    "layer_conv2, weights_conv2 = \\\n",
    "    new_conv_layer(input=layer_conv1,\n",
    "                   num_input_channels=num_filters1,\n",
    "                   filter_size=filter_size1,\n",
    "                   num_filters=num_filters1)\n",
    "\n",
    "# CONVOLUTIONAL LAYER 3\n",
    "\n",
    "layer_conv3, weights_conv3 = \\\n",
    "    new_conv_layer(input=layer_conv2,\n",
    "                   num_input_channels=num_filters1,\n",
    "                   filter_size=filter_size1,\n",
    "                   num_filters=num_filters1)\n",
    "\n",
    "# POOLING LAYER\n",
    "\n",
    "# Use pooling to down-sample the image resolution\n",
    "# This is 4x4 max-pooling\n",
    "layer_pool1 = tf.nn.max_pool(value=layer_conv3, ksize=[1, 2, 2, 1], \\\n",
    "                             strides=[1, 4, 4, 1], padding='SAME')\n",
    "\n",
    "# CONVOLUTIONAL LAYER 4\n",
    "\n",
    "layer_conv4, weights_conv4 = \\\n",
    "    new_conv_layer(input=layer_pool1,\n",
    "                   num_input_channels=num_filters1,\n",
    "                   filter_size=filter_size2,\n",
    "                   num_filters=num_filters2)\n",
    "\n",
    "\n",
    "# CONVOLUTIONAL LAYER 5\n",
    "\n",
    "layer_conv5, weights_conv5 = \\\n",
    "    new_conv_layer(input=layer_conv4,\n",
    "                   num_input_channels=num_filters2,\n",
    "                   filter_size=filter_size2,\n",
    "                   num_filters=num_filters2)\n",
    "\n",
    "# CONVOLUTIONAL LAYER 6\n",
    "\n",
    "layer_conv6, weights_conv6 = \\\n",
    "    new_conv_layer(input=layer_conv5,\n",
    "                   num_input_channels=num_filters2,\n",
    "                   filter_size=filter_size2,\n",
    "                   num_filters=num_filters2)\n",
    "\n",
    "# POOLING LAYER\n",
    "\n",
    "# Use pooling to down-sample the image resolution\n",
    "layer_pool2 = tf.nn.max_pool(value=layer_conv6, ksize=[1, 2, 2, 1], \\\n",
    "                             strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "drop = tf.nn.dropout(layer_pool2, 0.99) \n",
    "\n",
    "# FLATTEN LAYER\n",
    "\n",
    "layer_flat, num_features = flatten_layer(layer_pool2)\n",
    "\n",
    "# FULLY CONNECTED LAYER 1\n",
    "\n",
    "layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=fc_size,\n",
    "                         use_relu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the eucliean loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('euclidean_loss'):\n",
    "    squared_err = tf.squared_difference(layer_fc1, y_true)\n",
    "    with tf.name_scope('total'):\n",
    "        cost = tf.reduce_mean(squared_err)\n",
    "tf.summary.scalar('train_euclidean_loss', cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an exponential decay for the learning rate and an Adam Optimizer that happened to give the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('learning_rate_definition'):\n",
    "    global_step = tf.Variable(0.1, trainable=True)  # trainable=False\n",
    "    decay_epoch = 300\n",
    "    learning_rt = tf.train.exponential_decay(0.001, global_step,\n",
    "                                                np.floor(decay_epoch * 10),\n",
    "                                                0.95, staircase=True)\n",
    "tf.summary.scalar('learning_rate', learning_rt)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=learning_rt).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "On validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('validation_score'):\n",
    "    score = tf.reduce_mean(tf.squared_difference(layer_fc1, y_true))\n",
    "tf.summary.scalar('test_score', score)\n",
    "\n",
    "session = tf.Session()\n",
    "# Merge all the summaries\n",
    "merged = tf.summary.merge_all()\n",
    "# SUMMARIES\n",
    "train_writer = tf.summary.FileWriter(join(summaries_dir, 'train'), session.graph)\n",
    "val_writer = tf.summary.FileWriter(join(summaries_dir, 'validation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      "- Training-set:\n",
      "\timages=(100000, 2304)\tlabels=(100000, 128)\n",
      "- Validation-set:\n",
      "\timages=(10000, 2304)\tlabels=(10000, 128)\n",
      "- Test-set:\n",
      "\timages=(10000, 2304)\n",
      "Number of learnable parameters:  46669\n",
      "Optimization Iteration:      1, Test Score: 1.0802%\n",
      "Optimization Iteration:    101, Test Score: 0.7755%\n",
      "Optimization Iteration:    201, Test Score: 0.7751%\n",
      "Optimization Iteration:    301, Test Score: 0.7756%\n",
      "Optimization Iteration:    401, Test Score: 0.7737%\n",
      "Optimization Iteration:    501, Test Score: 0.7734%\n",
      "Optimization Iteration:    601, Test Score: 0.7730%\n",
      "Optimization Iteration:    701, Test Score: 0.7720%\n",
      "Optimization Iteration:    801, Test Score: 0.7717%\n",
      "Optimization Iteration:    901, Test Score: 0.7722%\n",
      "Optimization Iteration:   1001, Test Score: 0.7706%\n",
      "Optimization Iteration:   1101, Test Score: 0.7708%\n",
      "Optimization Iteration:   1201, Test Score: 0.7713%\n",
      "Optimization Iteration:   1301, Test Score: 0.7697%\n",
      "Optimization Iteration:   1401, Test Score: 0.7710%\n",
      "Optimization Iteration:   1501, Test Score: 0.7695%\n",
      "Optimization Iteration:   1601, Test Score: 0.7699%\n",
      "Optimization Iteration:   1701, Test Score: 0.7707%\n",
      "Optimization Iteration:   1801, Test Score: 0.7706%\n",
      "Optimization Iteration:   1901, Test Score: 0.7700%\n",
      "Optimization Iteration:   2001, Test Score: 0.7686%\n",
      "Optimization Iteration:   2101, Test Score: 0.7669%\n",
      "Optimization Iteration:   2201, Test Score: 0.7678%\n",
      "Optimization Iteration:   2301, Test Score: 0.7686%\n",
      "Optimization Iteration:   2401, Test Score: 0.7668%\n",
      "Optimization Iteration:   2501, Test Score: 0.7674%\n",
      "Optimization Iteration:   2601, Test Score: 0.7679%\n",
      "Optimization Iteration:   2701, Test Score: 0.7676%\n",
      "Optimization Iteration:   2801, Test Score: 0.7657%\n",
      "Optimization Iteration:   2901, Test Score: 0.7667%\n",
      "Optimization Iteration:   3001, Test Score: 0.7665%\n",
      "Optimization Iteration:   3101, Test Score: 0.7662%\n",
      "Optimization Iteration:   3201, Test Score: 0.7656%\n",
      "Optimization Iteration:   3301, Test Score: 0.7657%\n",
      "Optimization Iteration:   3401, Test Score: 0.7672%\n",
      "Optimization Iteration:   3501, Test Score: 0.7676%\n",
      "Optimization Iteration:   3601, Test Score: 0.7684%\n",
      "Optimization Iteration:   3701, Test Score: 0.7648%\n",
      "Optimization Iteration:   3801, Test Score: 0.7646%\n",
      "Optimization Iteration:   3901, Test Score: 0.7649%\n",
      "Optimization Iteration:   4001, Test Score: 0.7648%\n",
      "Optimization Iteration:   4101, Test Score: 0.7642%\n",
      "Optimization Iteration:   4201, Test Score: 0.7653%\n",
      "Optimization Iteration:   4301, Test Score: 0.7657%\n",
      "Optimization Iteration:   4401, Test Score: 0.7647%\n",
      "Optimization Iteration:   4501, Test Score: 0.7642%\n",
      "Optimization Iteration:   4601, Test Score: 0.7642%\n",
      "Optimization Iteration:   4701, Test Score: 0.7639%\n",
      "Optimization Iteration:   4801, Test Score: 0.7658%\n",
      "Optimization Iteration:   4901, Test Score: 0.7649%\n",
      "Optimization Iteration:   5001, Test Score: 0.7644%\n",
      "Optimization Iteration:   5101, Test Score: 0.7651%\n",
      "Optimization Iteration:   5201, Test Score: 0.7642%\n",
      "Optimization Iteration:   5301, Test Score: 0.7633%\n",
      "Optimization Iteration:   5401, Test Score: 0.7633%\n",
      "Optimization Iteration:   5501, Test Score: 0.7642%\n",
      "Optimization Iteration:   5601, Test Score: 0.7648%\n",
      "Optimization Iteration:   5701, Test Score: 0.7626%\n",
      "Optimization Iteration:   5801, Test Score: 0.7635%\n",
      "Optimization Iteration:   5901, Test Score: 0.7638%\n",
      "Optimization Iteration:   6001, Test Score: 0.7654%\n",
      "Optimization Iteration:   6101, Test Score: 0.7630%\n",
      "Optimization Iteration:   6201, Test Score: 0.7635%\n",
      "Optimization Iteration:   6301, Test Score: 0.7627%\n",
      "Optimization Iteration:   6401, Test Score: 0.7656%\n",
      "Optimization Iteration:   6501, Test Score: 0.7651%\n",
      "Optimization Iteration:   6601, Test Score: 0.7646%\n",
      "Optimization Iteration:   6701, Test Score: 0.7626%\n",
      "Optimization Iteration:   6801, Test Score: 0.7658%\n",
      "Optimization Iteration:   6901, Test Score: 0.7642%\n",
      "Optimization Iteration:   7001, Test Score: 0.7629%\n",
      "Optimization Iteration:   7101, Test Score: 0.7635%\n",
      "Optimization Iteration:   7201, Test Score: 0.7639%\n",
      "Optimization Iteration:   7301, Test Score: 0.7633%\n",
      "Optimization Iteration:   7401, Test Score: 0.7629%\n",
      "Optimization Iteration:   7501, Test Score: 0.7617%\n",
      "Optimization Iteration:   7601, Test Score: 0.7630%\n",
      "Optimization Iteration:   7701, Test Score: 0.7632%\n",
      "Optimization Iteration:   7801, Test Score: 0.7625%\n",
      "Optimization Iteration:   7901, Test Score: 0.7628%\n",
      "Optimization Iteration:   8001, Test Score: 0.7630%\n",
      "Optimization Iteration:   8101, Test Score: 0.7636%\n",
      "Optimization Iteration:   8201, Test Score: 0.7629%\n",
      "Optimization Iteration:   8301, Test Score: 0.7639%\n",
      "Optimization Iteration:   8401, Test Score: 0.7625%\n",
      "Optimization Iteration:   8501, Test Score: 0.7640%\n",
      "Optimization Iteration:   8601, Test Score: 0.7638%\n",
      "Optimization Iteration:   8701, Test Score: 0.7624%\n",
      "Optimization Iteration:   8801, Test Score: 0.7615%\n",
      "Optimization Iteration:   8901, Test Score: 0.7629%\n",
      "Optimization Iteration:   9001, Test Score: 0.7629%\n",
      "Optimization Iteration:   9101, Test Score: 0.7618%\n",
      "Optimization Iteration:   9201, Test Score: 0.7624%\n",
      "Optimization Iteration:   9301, Test Score: 0.7631%\n",
      "Optimization Iteration:   9401, Test Score: 0.7616%\n",
      "Optimization Iteration:   9501, Test Score: 0.7642%\n",
      "Optimization Iteration:   9601, Test Score: 0.7618%\n",
      "Optimization Iteration:   9701, Test Score: 0.7629%\n",
      "Optimization Iteration:   9801, Test Score: 0.7617%\n",
      "Optimization Iteration:   9901, Test Score: 0.7627%\n",
      "Optimization Iteration:  10001, Test Score: 0.7613%\n",
      "Optimization Iteration:  10101, Test Score: 0.7627%\n",
      "Optimization Iteration:  10201, Test Score: 0.7624%\n",
      "Optimization Iteration:  10301, Test Score: 0.7629%\n",
      "Optimization Iteration:  10401, Test Score: 0.7601%\n",
      "Optimization Iteration:  10501, Test Score: 0.7631%\n",
      "Optimization Iteration:  10601, Test Score: 0.7627%\n",
      "Optimization Iteration:  10701, Test Score: 0.7617%\n",
      "Optimization Iteration:  10801, Test Score: 0.7622%\n",
      "Optimization Iteration:  10901, Test Score: 0.7618%\n",
      "Optimization Iteration:  11001, Test Score: 0.7619%\n",
      "Optimization Iteration:  11101, Test Score: 0.7614%\n",
      "Optimization Iteration:  11201, Test Score: 0.7631%\n",
      "Optimization Iteration:  11301, Test Score: 0.7634%\n",
      "Optimization Iteration:  11401, Test Score: 0.7625%\n",
      "Optimization Iteration:  11501, Test Score: 0.7614%\n",
      "Optimization Iteration:  11601, Test Score: 0.7632%\n",
      "Optimization Iteration:  11701, Test Score: 0.7640%\n",
      "Optimization Iteration:  11801, Test Score: 0.7650%\n",
      "Optimization Iteration:  11901, Test Score: 0.7630%\n",
      "Optimization Iteration:  12001, Test Score: 0.7635%\n",
      "Optimization Iteration:  12101, Test Score: 0.7617%\n",
      "Optimization Iteration:  12201, Test Score: 0.7634%\n",
      "Optimization Iteration:  12301, Test Score: 0.7625%\n",
      "Optimization Iteration:  12401, Test Score: 0.7617%\n",
      "Optimization Iteration:  12501, Test Score: 0.7674%\n",
      "Optimization Iteration:  12601, Test Score: 0.7613%\n",
      "Optimization Iteration:  12701, Test Score: 0.7629%\n",
      "Optimization Iteration:  12801, Test Score: 0.7610%\n",
      "Optimization Iteration:  12901, Test Score: 0.7603%\n",
      "Optimization Iteration:  13001, Test Score: 0.7621%\n",
      "Optimization Iteration:  13101, Test Score: 0.7626%\n",
      "Optimization Iteration:  13201, Test Score: 0.7622%\n",
      "Optimization Iteration:  13301, Test Score: 0.7606%\n",
      "Optimization Iteration:  13401, Test Score: 0.7622%\n",
      "Optimization Iteration:  13501, Test Score: 0.7616%\n",
      "Optimization Iteration:  13601, Test Score: 0.7628%\n",
      "Optimization Iteration:  13701, Test Score: 0.7613%\n",
      "Optimization Iteration:  13801, Test Score: 0.7625%\n",
      "Optimization Iteration:  13901, Test Score: 0.7606%\n",
      "Optimization Iteration:  14001, Test Score: 0.7639%\n",
      "Optimization Iteration:  14101, Test Score: 0.7632%\n",
      "Optimization Iteration:  14201, Test Score: 0.7637%\n",
      "Optimization Iteration:  14301, Test Score: 0.7653%\n",
      "Optimization Iteration:  14401, Test Score: 0.7625%\n",
      "Optimization Iteration:  14501, Test Score: 0.7613%\n",
      "Optimization Iteration:  14601, Test Score: 0.7614%\n",
      "Optimization Iteration:  14701, Test Score: 0.7619%\n",
      "Optimization Iteration:  14801, Test Score: 0.7615%\n",
      "Optimization Iteration:  14901, Test Score: 0.7630%\n",
      "Optimization Iteration:  15001, Test Score: 0.7620%\n",
      "Optimization Iteration:  15101, Test Score: 0.7599%\n",
      "Optimization Iteration:  15201, Test Score: 0.7634%\n",
      "Optimization Iteration:  15301, Test Score: 0.7620%\n",
      "Optimization Iteration:  15401, Test Score: 0.7637%\n",
      "Optimization Iteration:  15501, Test Score: 0.7632%\n",
      "Optimization Iteration:  15601, Test Score: 0.7627%\n",
      "Optimization Iteration:  15701, Test Score: 0.7616%\n",
      "Optimization Iteration:  15801, Test Score: 0.7675%\n",
      "Optimization Iteration:  15901, Test Score: 0.7624%\n",
      "Optimization Iteration:  16001, Test Score: 0.7609%\n",
      "Optimization Iteration:  16101, Test Score: 0.7621%\n",
      "Optimization Iteration:  16201, Test Score: 0.7616%\n",
      "Optimization Iteration:  16301, Test Score: 0.7632%\n",
      "Optimization Iteration:  16401, Test Score: 0.7659%\n",
      "Optimization Iteration:  16501, Test Score: 0.7635%\n",
      "Optimization Iteration:  16601, Test Score: 0.7630%\n",
      "Optimization Iteration:  16701, Test Score: 0.7617%\n",
      "Optimization Iteration:  16801, Test Score: 0.7621%\n",
      "Optimization Iteration:  16901, Test Score: 0.7639%\n",
      "Optimization Iteration:  17001, Test Score: 0.7627%\n",
      "Optimization Iteration:  17101, Test Score: 0.7624%\n",
      "Optimization Iteration:  17201, Test Score: 0.7614%\n",
      "Optimization Iteration:  17301, Test Score: 0.7627%\n",
      "Optimization Iteration:  17401, Test Score: 0.7633%\n",
      "Optimization Iteration:  17501, Test Score: 0.7626%\n",
      "Optimization Iteration:  17601, Test Score: 0.7609%\n",
      "Optimization Iteration:  17701, Test Score: 0.7609%\n",
      "Optimization Iteration:  17801, Test Score: 0.7611%\n",
      "Optimization Iteration:  17901, Test Score: 0.7629%\n",
      "Optimization Iteration:  18001, Test Score: 0.7595%\n",
      "Optimization Iteration:  18101, Test Score: 0.7608%\n",
      "Optimization Iteration:  18201, Test Score: 0.7629%\n",
      "Optimization Iteration:  18301, Test Score: 0.7604%\n",
      "Optimization Iteration:  18401, Test Score: 0.7621%\n",
      "Optimization Iteration:  18501, Test Score: 0.7622%\n",
      "Optimization Iteration:  18601, Test Score: 0.7615%\n",
      "Optimization Iteration:  18701, Test Score: 0.7618%\n",
      "Optimization Iteration:  18801, Test Score: 0.7631%\n",
      "Optimization Iteration:  18901, Test Score: 0.7621%\n",
      "Optimization Iteration:  19001, Test Score: 0.7634%\n",
      "Optimization Iteration:  19101, Test Score: 0.7625%\n",
      "Optimization Iteration:  19201, Test Score: 0.7633%\n",
      "Optimization Iteration:  19301, Test Score: 0.7635%\n",
      "Optimization Iteration:  19401, Test Score: 0.7621%\n",
      "Optimization Iteration:  19501, Test Score: 0.7639%\n",
      "Optimization Iteration:  19601, Test Score: 0.7618%\n",
      "Optimization Iteration:  19701, Test Score: 0.7606%\n",
      "Optimization Iteration:  19801, Test Score: 0.7623%\n",
      "Optimization Iteration:  19901, Test Score: 0.7630%\n",
      "Optimization Iteration:  20001, Test Score: 0.7636%\n",
      "Optimization Iteration:  20101, Test Score: 0.7618%\n",
      "Optimization Iteration:  20201, Test Score: 0.7625%\n",
      "Optimization Iteration:  20301, Test Score: 0.7610%\n",
      "Optimization Iteration:  20401, Test Score: 0.7641%\n",
      "Optimization Iteration:  20501, Test Score: 0.7643%\n",
      "Optimization Iteration:  20601, Test Score: 0.7619%\n",
      "Optimization Iteration:  20701, Test Score: 0.7613%\n",
      "Optimization Iteration:  20801, Test Score: 0.7619%\n",
      "Optimization Iteration:  20901, Test Score: 0.7630%\n",
      "Optimization Iteration:  21001, Test Score: 0.7613%\n",
      "Optimization Iteration:  21101, Test Score: 0.7627%\n",
      "Optimization Iteration:  21201, Test Score: 0.7618%\n",
      "Optimization Iteration:  21301, Test Score: 0.7624%\n",
      "Optimization Iteration:  21401, Test Score: 0.7628%\n",
      "Optimization Iteration:  21501, Test Score: 0.7618%\n",
      "Optimization Iteration:  21601, Test Score: 0.7615%\n",
      "Optimization Iteration:  21701, Test Score: 0.7630%\n",
      "Optimization Iteration:  21801, Test Score: 0.7620%\n",
      "Optimization Iteration:  21901, Test Score: 0.7618%\n",
      "Optimization Iteration:  22001, Test Score: 0.7621%\n",
      "Optimization Iteration:  22101, Test Score: 0.7641%\n",
      "Optimization Iteration:  22201, Test Score: 0.7625%\n",
      "Optimization Iteration:  22301, Test Score: 0.7619%\n",
      "Optimization Iteration:  22401, Test Score: 0.7609%\n",
      "Optimization Iteration:  22501, Test Score: 0.7632%\n",
      "Optimization Iteration:  22601, Test Score: 0.7630%\n",
      "Optimization Iteration:  22701, Test Score: 0.7609%\n",
      "Optimization Iteration:  22801, Test Score: 0.7623%\n",
      "Optimization Iteration:  22901, Test Score: 0.7640%\n",
      "Optimization Iteration:  23001, Test Score: 0.7612%\n",
      "Optimization Iteration:  23101, Test Score: 0.7631%\n",
      "Optimization Iteration:  23201, Test Score: 0.7647%\n",
      "Optimization Iteration:  23301, Test Score: 0.7625%\n",
      "Optimization Iteration:  23401, Test Score: 0.7628%\n",
      "Optimization Iteration:  23501, Test Score: 0.7611%\n",
      "Optimization Iteration:  23601, Test Score: 0.7637%\n",
      "Optimization Iteration:  23701, Test Score: 0.7606%\n",
      "Optimization Iteration:  23801, Test Score: 0.7615%\n",
      "Optimization Iteration:  23901, Test Score: 0.7629%\n",
      "Optimization Iteration:  24001, Test Score: 0.7618%\n",
      "Optimization Iteration:  24101, Test Score: 0.7641%\n",
      "Optimization Iteration:  24201, Test Score: 0.7616%\n",
      "Optimization Iteration:  24301, Test Score: 0.7625%\n",
      "Optimization Iteration:  24401, Test Score: 0.7618%\n",
      "Optimization Iteration:  24501, Test Score: 0.7636%\n",
      "Optimization Iteration:  24601, Test Score: 0.7626%\n",
      "Optimization Iteration:  24701, Test Score: 0.7613%\n",
      "Optimization Iteration:  24801, Test Score: 0.7629%\n",
      "Optimization Iteration:  24901, Test Score: 0.7615%\n",
      "Optimization Iteration:  25001, Test Score: 0.7643%\n",
      "Optimization Iteration:  25101, Test Score: 0.7617%\n",
      "Optimization Iteration:  25201, Test Score: 0.7625%\n",
      "Optimization Iteration:  25301, Test Score: 0.7645%\n",
      "Optimization Iteration:  25401, Test Score: 0.7614%\n",
      "Optimization Iteration:  25501, Test Score: 0.7619%\n",
      "Optimization Iteration:  25601, Test Score: 0.7616%\n",
      "Optimization Iteration:  25701, Test Score: 0.7620%\n",
      "Optimization Iteration:  25801, Test Score: 0.7624%\n",
      "Optimization Iteration:  25901, Test Score: 0.7612%\n",
      "Optimization Iteration:  26001, Test Score: 0.7620%\n",
      "Optimization Iteration:  26101, Test Score: 0.7618%\n",
      "Optimization Iteration:  26201, Test Score: 0.7623%\n",
      "Optimization Iteration:  26301, Test Score: 0.7617%\n",
      "Optimization Iteration:  26401, Test Score: 0.7611%\n",
      "Optimization Iteration:  26501, Test Score: 0.7645%\n",
      "Optimization Iteration:  26601, Test Score: 0.7596%\n",
      "Optimization Iteration:  26701, Test Score: 0.7598%\n",
      "Optimization Iteration:  26801, Test Score: 0.7624%\n",
      "Optimization Iteration:  26901, Test Score: 0.7630%\n",
      "Optimization Iteration:  27001, Test Score: 0.7624%\n",
      "Optimization Iteration:  27101, Test Score: 0.7634%\n",
      "Optimization Iteration:  27201, Test Score: 0.7606%\n",
      "Optimization Iteration:  27301, Test Score: 0.7606%\n",
      "Optimization Iteration:  27401, Test Score: 0.7622%\n",
      "Optimization Iteration:  27501, Test Score: 0.7612%\n",
      "Optimization Iteration:  27601, Test Score: 0.7597%\n",
      "Optimization Iteration:  27701, Test Score: 0.7634%\n",
      "Optimization Iteration:  27801, Test Score: 0.7614%\n",
      "Optimization Iteration:  27901, Test Score: 0.7619%\n",
      "Optimization Iteration:  28001, Test Score: 0.7629%\n",
      "Optimization Iteration:  28101, Test Score: 0.7634%\n",
      "Optimization Iteration:  28201, Test Score: 0.7625%\n",
      "Optimization Iteration:  28301, Test Score: 0.7631%\n",
      "Optimization Iteration:  28401, Test Score: 0.7616%\n",
      "Optimization Iteration:  28501, Test Score: 0.7630%\n",
      "Optimization Iteration:  28601, Test Score: 0.7614%\n",
      "Optimization Iteration:  28701, Test Score: 0.7612%\n",
      "Optimization Iteration:  28801, Test Score: 0.7618%\n",
      "Optimization Iteration:  28901, Test Score: 0.7610%\n",
      "Optimization Iteration:  29001, Test Score: 0.7617%\n",
      "Optimization Iteration:  29101, Test Score: 0.7607%\n",
      "Optimization Iteration:  29201, Test Score: 0.7626%\n",
      "Optimization Iteration:  29301, Test Score: 0.7606%\n",
      "Optimization Iteration:  29401, Test Score: 0.7644%\n",
      "Optimization Iteration:  29501, Test Score: 0.7604%\n",
      "Optimization Iteration:  29601, Test Score: 0.7608%\n",
      "Optimization Iteration:  29701, Test Score: 0.7609%\n",
      "Optimization Iteration:  29801, Test Score: 0.7614%\n",
      "Optimization Iteration:  29901, Test Score: 0.7606%\n",
      "Time usage: 1:00:31\n",
      "Score on Test-Set: 0.7617%\n"
     ]
    }
   ],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "train_batch_size = 64\n",
    "# Counter for total number of iterations performed so far.\n",
    "total_iterations = 0\n",
    "\n",
    "def feed_dict(train):\n",
    "    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "    if train:\n",
    "        xs, ys = next_batch(train_batch_size,\n",
    "                            train_image_data,\n",
    "                            train_template_data)\n",
    "        # k = 0.8\n",
    "    else:\n",
    "        xs, ys = valid_image_data, valid_template_data\n",
    "        # k = 1.0\n",
    "    return {x: xs, y_true: ys}  # , keep_prob: k}\n",
    "\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    # Ensure we update the global variable rather than a local copy.\n",
    "    global total_iterations\n",
    "\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iterations):\n",
    "\n",
    "        # Every 1000 iterations:\n",
    "        #   Record summaries and test-set score\n",
    "        #   Print Test Score\n",
    "        if i % 100 == 0:\n",
    "            # Calculate the accuracy on the validation set.\n",
    "            summary, sc = session.run([merged, score], feed_dict=feed_dict(False))\n",
    "            val_writer.add_summary(summary, i)\n",
    "            # Message for printing.\n",
    "            msg = \"Optimization Iteration: {0:>6}, Test Score: {1:.4%}\"\n",
    "            # Print it.\n",
    "            print(msg.format(i + 1, sc))\n",
    "\n",
    "        else:\n",
    "            # Run the optimizer using this batch of training data.\n",
    "            # TensorFlow assigns the variables with feed_dict function\n",
    "            # to the placeholder variables and then runs the optimizer.\n",
    "            summary, _ = session.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "            train_writer.add_summary(summary, i)\n",
    "\n",
    "    # Update the total number of iterations performed.\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "\n",
    "\n",
    "print(\"Number of learnable parameters: \", get_total_param())\n",
    "try:\n",
    "    optimize(num_iterations=30000)\n",
    "    val_pred = cnn_predict(valid_image_data, valid_template_data)\n",
    "    test_pred = cnn_predict(test_image_data)\n",
    "\n",
    "    with open('template_pred.bin', 'wb') as f:\n",
    "        for i in range(num_test_images):\n",
    "            f.write(test_pred[i, :])\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    val_writer.close()\n",
    "    session.close()\n",
    "    train_writer.close()\n",
    "\n",
    "f.close()\n",
    "train_writer.close()\n",
    "val_writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: tf_model\\cnn_ckpt\n"
     ]
    }
   ],
   "source": [
    "# Save the variables to disk.\n",
    "save_path = saver.save(session, join(summaries_dir, \"tf_model\", \"cnn_ckpt\"))\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
